## Efficiently Modeling Long Sequences with Structured State Spaces

Albert Gu, Karan Goel, and Christopher R´e


Department of Computer Science, Stanford University


_{_ `albertgu,krng` _}_ `@stanford.edu`, `chrismre@cs.stanford.edu`


**Abstract**


A central goal of sequence modeling is designing a single principled model that can address sequence
data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional
models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies,
they still struggle to scale to very long sequences of 10000 or more steps. A promising recent approach
proposed modeling sequences by simulating the fundamental state space model (SSM) _x_ _[′]_ ( _t_ ) = _Ax_ ( _t_ ) +
_Bu_ ( _t_ ) _, y_ ( _t_ ) = _Cx_ ( _t_ ) + _Du_ ( _t_ ), and showed that for appropriate choices of the state matrix _A_, this
system could handle long-range dependencies mathematically and empirically. However, this method has
prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling
solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for
the SSM, and show that it can be computed much more efficiently than prior approaches while preserving
their theoretical strengths. Our technique involves conditioning _A_ with a low-rank correction, allowing
it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel.
S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91%
accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger
2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks,
while performing generation 60 _×_ faster (iii) SoTA on every task from the Long Range Arena benchmark,
including solving the challenging Path-X task of length 16k that all prior work fails on, while being as
efficient as all competitors. [1]

##### **1 Introduction**


A central problem in sequence modeling is efficiently handling data that contains long-range dependencies
(LRDs). Real-world time-series data often requires reasoning over tens of thousands of time steps, while few
sequence models address even thousands of time steps. For instance, results from the long-range arena (LRA)
benchmark [40] highlight that sequence models today perform poorly on LRD tasks, including one (Path-X)
where no model performs better than random guessing.


Since LRDs are perhaps the foremost challenge for sequence models, all standard model families such as
continuous-time models (CTMs), RNNs, CNNs, and Transformers include many specialized variants designed
to address them. Modern examples include orthogonal and Lipschitz RNNs [1, 13] to combat vanishing
gradients, dilated convolutions to increase context size [3, 28], and an increasingly vast family of efficient
Transformers that reduce the quadratic dependence on sequence length [8, 22]. Despite being designed
for LRDs, these solutions still perform poorly on challenging benchmarks such as LRA [40] or raw audio
classification [18].


An alternative approach to LRDs was recently introduced based on the **state** **space** **model** **(SSM)** (Fig. 1).
SSMs are a foundational scientific model used in fields such as control theory, computational neuroscience,
and many more, but have not been applicable to deep learning for concrete theoretical reasons. In particular,
Gu et al. [18] showed that deep SSMs actually struggle even on simple tasks, but can perform exceptionally


1Code is publicly available at `[https://github.com/HazyResearch/state-spaces](https://github.com/HazyResearch/state-spaces)` .


1


Figure 1: ( **Left** ) State Space Models (SSM) parameterized by matrices _**A**_ _,_ _**B**_ _,_ _**C**_ _,_ _**D**_ map an input signal _u_ ( _t_ ) to
output _y_ ( _t_ ) through a latent state _x_ ( _t_ ). ( **Center** ) Recent theory on continuous-time memorization derives special
_**A**_ matrices that allow SSMs to capture LRDs mathematically and empirically. ( **Right** ) SSMs can be computed
either as a recurrence (left) or convolution (right). However, materializing these conceptual views requires utilizing
different representations of its parameters (red, blue, green) which are very expensive to compute. S4 introduces a
novel parameterization that efficiently swaps between these representations, allowing it to handle a wide range of
tasks, be efficient at both training and inference, and excel at long sequences.


well when equipped with special state matrices _**A**_ recently derived to solve a problem of continuous-time
memorization [16, 45]. Their Linear State Space Layer (LSSL) conceptually unifies the strengths of CTM,
RNN and CNN models, and provides a proof of concept that deep SSMs can address LRDs in principle.


Unfortunately, the LSSL is infeasible to use in practice because of prohibitive computation and memory
requirements induced by the state representation. For state dimension _N_ and sequence length _L_, computing
the latent state requires _O_ ( _N_ [2] _L_ ) operations and _O_ ( _NL_ ) space - compared to a Ω( _L_ + _N_ ) lower bound for
both. Thus for reasonably sized models (e.g. _N_ = 256 in Gu et al. [18]), the LSSL uses orders of magnitude
more memory than comparably-sized RNNs or CNNs. Although theoretically efficient algorithms for the
LSSL were proposed, we show that these are numerically unstable. In particular, the special _**A**_ matrix is
highly non-normal in the linear algebraic sense, which prevents the application of conventional algorithmic
techniques. Consequently, although the LSSL showed that SSMs have strong performance, they are currently
computationally impractical as a general sequence modeling solution.


In this work, we introduce the **Structured** **State** **Space** **(S4)** sequence model based on the SSM that solves
the critical computational bottleneck in previous work. Technically, S4 reparameterizes the structured state
matrices _**A**_ appearing in Gu et al. [16], Voelker et al. [45] by decomposing them as the sum of a low-rank
and normal term. Additionally, instead of expanding the standard SSM in coefficient space, we compute its
truncated generating function in frequency space, which can be simplified into a multipole-like evaluation.
Combining these two ideas, we show that the low-rank term can be corrected by the Woodbury identity
while the normal term can be diagonalized stably, ultimately reducing to a well-studied and theoretically
stable Cauchy kernel [29, 30]. This results in _O_ [˜] ( _N_ + _L_ ) computation and _O_ ( _N_ + _L_ ) memory usage, which is
essentially tight for sequence models. Compared to the LSSL, S4 is up to 30 _×_ faster with 400 _×_ less memory
usage, while exceeding the LSSL’s performance empirically.


Empirically, S4 significantly advances the state-of-the-art for LRD. On the LRA benchmark for efficient
sequence models, S4 is as fast as all baselines while outperforming them by 20+ points on average. S4 is the
first model to solve the difficult LRA Path-X task (length-16384), achieving **88%** **accuracy** **compared** **to**
**50%** **random** **guessing** for all prior work. On speech classification with length-16000 sequences, S4 halves
the test error (1 _._ 7%) of specialized Speech CNNs - by contrast, all RNN and Transformer baselines fail to
learn ( _≥_ 70% error).


**Towards** **a** **general-purpose** **sequence** **model.** Beyond LRD, a broad goal of machine learning is to
develop a single model that can be used across a wide range of problems. Models today are typically


2


specialized to solve problems from a particular domain (e.g. images, audio, text, time-series), and enable a
narrow range of capabilities (e.g. efficient training, fast generation, handling irregularly sampled data). This
specialization is typically expressed via domain-specific preprocessing, inductive biases, and architectures.
Sequence models provide a general framework for solving many of these problems with reduced specialization

- e.g. Vision Transformers for image classification with less 2D information [12]. However, most models such
as Transformers generally still require substantial specialization per task to achieve high performance.


Deep SSMs in particular have conceptual strengths that suggest they may be promising as a general sequence
modeling solution. These strengths include a principled approach to handling LRDs, as well as the ability
to move between continuous-time, convolutional, and recurrent model representations, each with distinct
capabilities (Fig. 1). Our technical contributions enable SSMs to be applied successfully to a varied set of
benchmarks with minimal modification:


- _Large-scale_ _generative_ _modeling._ On CIFAR-10 density estimation, S4 is competitive with the best
autoregressive models (2 _._ 85 bits per dim). On WikiText-103 language modeling, S4 substantially closes the
gap to Transformers (within 0 _._ 8 perplexity), setting SoTA for attention-free models.


- _Fast_ _autoregressive_ _generation._ Like RNNs, S4 can use its latent state to perform 60 _×_ faster pixel/token
generation than standard autoregressive models on CIFAR-10 and WikiText-103.


- _Sampling_ _resolution_ _change._ Like specialized CTMs, S4 can adapt to changes in time-series sampling
frequency without retraining, e.g. at 0 _._ 5 _×_ frequency on speech classification.


- _Learning_ _with_ _weaker_ _inductive_ _biases._ With no architectural changes, S4 surpasses Speech CNNs on speech
classification, outperforms the specialized Informer model on time-series forecasting problems, and matches
a 2-D ResNet on sequential CIFAR with over 90% accuracy.

##### **2 Background: State Spaces**


Sections 2.1 to 2.4 describe the four properties of SSMs in Fig. 1: the classic continuous-time representation,
addressing LRDs with the HiPPO framework, the discrete-time recurrent representation, and the parallelizable
convolution representation. In particular, Section 2.4 introduces the SSM convolution kernel _**K**_, which is the
focus of our theoretical contributions in Section 3.


**2.1** **State** **Space** **Models:** **A** **Continuous-time** **Latent** **State** **Model**


The state space model is defined by the simple equation (1). It maps a 1-D input signal _u_ ( _t_ ) to an _N_ -D
latent state _x_ ( _t_ ) before projecting to a 1-D output signal _y_ ( _t_ ).


_x_ _[′]_ ( _t_ ) = _**A**_ _x_ ( _t_ ) + _**B**_ _u_ ( _t_ )
(1)
_y_ ( _t_ ) = _**C**_ _x_ ( _t_ ) + _**D**_ _u_ ( _t_ )


SSMs are broadly used in many scientific disciplines and related to latent state models such as Hidden Markov
Models (HMM). Our goal is to simply use the SSM as a black-box representation in a deep sequence model,
where _**A**_ _,_ _**B**_ _,_ _**C**_ _,_ _**D**_ are parameters learned by gradient descent. For the remainder of this paper, we will omit
the parameter _**D**_ for exposition (or equivalently, assume _**D**_ = 0) because the term _**D**_ _u_ can be viewed as a
skip connection and is easy to compute.


**2.2** **Addressing** **Long-Range** **Dependencies** **with** **HiPPO**


Prior work found that the basic SSM (1) actually performs very poorly in practice. Intuitively, one explanation
is that linear first-order ODEs solve to an exponential function, and thus may suffer from gradients scaling
exponentially in the sequence length (i.e., the vanishing/exploding gradients problem [32]). To address this


3


problem, the LSSL leveraged the HiPPO theory of continuous-time memorization [16]. HiPPO specifies a
class of certain matrices _**A**_ _∈_ R _[N]_ _[×][N]_ that when incorporated into (1), allows the state _x_ ( _t_ ) to memorize the
history of the input _u_ ( _t_ ). The most important matrix in this class is defined by equation (2), which we will
call the HiPPO matrix. For example, the LSSL found that simply modifying an SSM from a random matrix
_**A**_ to equation (2) improved its performance on the sequential MNIST benchmark from 60% to 98%.



( **HiPPO** **Matrix** ) _**A**_ _nk_ = _−_








(2 _n_ + 1) [1] _[/]_ [2] (2 _k_ + 1) [1] _[/]_ [2] if _n > k_

_n_ + 1 if _n_ = _k_ _._ (2)

0 if _n < k_







**2.3** **Discrete-time** **SSM:** **The** **Recurrent** **Representation**


To be applied on a discrete input sequence ( _u_ 0 _, u_ 1 _, . . ._ ) instead of continuous function _u_ ( _t_ ), (1) must be
discretized by a **step** **size** ∆that represents the resolution of the input. Conceptually, the inputs _uk_ can be
viewed as sampling an implicit underlying continuous signal _u_ ( _t_ ), where _uk_ = _u_ ( _k_ ∆).


To discretize the continuous-time SSM, we follow prior work in using the bilinear method [43], which converts
the state matrix _**A**_ into an approximation _**A**_ . The discrete SSM is


_xk_ = _**A**_ _xk−_ 1 + _**B**_ _uk_ _**A**_ = ( _**I**_ _−_ ∆ _/_ 2 _·_ _**A**_ ) _[−]_ [1] ( _**I**_ + ∆ _/_ 2 _·_ _**A**_ )

(3)
_yk_ = _**C**_ _xk_ _**B**_ = ( _**I**_ _−_ ∆ _/_ 2 _·_ _**A**_ ) _[−]_ [1] ∆ _**B**_ _**C**_ = _**C**_ _._


Equation (3) is now a _sequence-to-sequence_ map _uk_ _�→_ _yk_ instead of function-to-function. Moreover the state
equation is now a recurrence in _xk_, allowing the discrete SSM to be computed like an RNN. Concretely,
_xk_ _∈_ R _[N]_ can be viewed as a _hidden_ _state_ with transition matrix _**A**_ .


Notationally, throughout this paper we use _**A**_ _,_ _**B**_ _, . . ._ to denote discretized SSM matrices defined by (3).
Note that these matrices are a function of both _**A**_ as well as a step size ∆; we suppress this dependence for
notational convenience when it is clear.


**2.4** **Training** **SSMs:** **The** **Convolutional** **Representation**


The recurrent SSM (3) is not practical for training on modern hardware due to its sequentiality. Instead, there
is a well-known connection between linear time-invariant (LTI) SSMs such as (1) and continuous convolutions.
Correspondingly, (3) can actually be written as a discrete convolution.


For simplicity let the initial state be _x−_ 1 = 0. Then unrolling (3) explicitly yields


~~2~~
_x_ 0 = _**B**_ _u_ 0 _x_ 1 = _**AB**_ _u_ 0 + _**B**_ _u_ 1 _x_ 2 = _**A**_ _**B**_ _u_ 0 + _**AB**_ _u_ 1 + _**B**_ _u_ 2 _. . ._


~~2~~
_y_ 0 = _**CB**_ _u_ 0 _y_ 1 = _**CAB**_ _u_ 0 + _**CB**_ _u_ 1 _y_ 2 = _**CA**_ _**B**_ _u_ 0 + _**CAB**_ _u_ 1 + _**CB**_ _u_ 2 _. . ._


This can be vectorized into a convolution (4) with an explicit formula for the convolution kernel (5).


~~_k_~~ ~~_k_~~ _−_ 1
_yk_ = _**CA**_ _**B**_ _u_ 0 + _**CA**_ _**B**_ _u_ 1 + _· · ·_ + _**CAB**_ _uk−_ 1 + _**CB**_ _uk_
(4)
_y_ = _**K**_ _∗_ _u._




           - ~~_i_~~           - ~~_L_~~ _−_ 1
_**K**_ _∈_ R _[L]_ := _KL_ ( _**A**_ _,_ _**B**_ _,_ _**C**_ ) := _**CA**_ _**B**_ _**B**_ ) _._ (5)

_i∈_ [ _L_ ] [= (] _**[CB]**_ _[,]_ _**[ CAB]**_ _[, . . .,]_ _**[ CA]**_



In other words, equation (4) is a single (non-circular) convolution and can be computed very efficiently with
FFTs, _provided_ that _**K**_ is known. However, computing _**K**_ in (5) is non-trivial and is the focus of our technical
contributions in Section 3. We call _**K**_ the **SSM** **convolution** **kernel** or filter.


4


##### **3 Method: Structured State Spaces (S4)**

Our technical results focus on developing the S4 parameterization and showing how to efficiently compute
all views of the SSM (Section 2): the continuous representation ( _**A**_ _,_ _**B**_ _,_ _**C**_ ) (1), the recurrent representation
( _**A**_ _,_ _**B**_ _,_ _**C**_ ) (3), and the convolutional representation _**K**_ (4).


Section 3.1 motivates our approach, which is based on the linear algebraic concepts of conjugation and
diagonalization, and discusses why the naive application of this approach does not work. Section 3.2 gives
an overview of the key technical components of our approach and formally defines the S4 parameterization.
Section 3.3 sketches the main results, showing that S4 is asymptotically efficient (up to log factors) for
sequence models. Proofs are in Appendices B and C.


**3.1** **Motivation:** **Diagonalization**


The fundamental bottleneck in computing the discrete-time SSM (3) is that it involves repeated matrix
multiplication by _**A**_ . For example, computing (5) naively as in the LSSL involves _L_ successive multiplications
by _**A**_, requiring _O_ ( _N_ [2] _L_ ) operations and _O_ ( _NL_ ) space.


To overcome this bottleneck, we use a structural result that allows us to simplify SSMs.


**Lemma** **3.1.** _Conjugation_ _is_ _an_ _equivalence_ _relation_ _on_ _SSMs_ ( _**A**_ _,_ _**B**_ _,_ _**C**_ ) _∼_ ( _**V**_ _[−]_ [1] _**AV**_ _,_ _**V**_ _[−]_ [1] _**B**_ _,_ _**CV**_ ) _._


_Proof._ Write out the two SSMs with state denoted by _x_ and _x_ ˜ respectively:


_x_ _[′]_ = _**A**_ _x_ + _**B**_ _u_ _x_ ˜ _[′]_ = _**V**_ _[−]_ [1] _**AV**_ _x_ ˜ + _**V**_ _[−]_ [1] _**B**_ _u_


_y_ = _**C**_ _x_ _y_ = _**CV**_ _x_ ˜


After multiplying the right side SSM by _**V**_, the two SSMs become identical with _x_ = _**V**_ _x_ ˜. Therefore these
compute the exact same operator _u �→_ _y_, but with a change of basis by _**V**_ in the state _x_ .


Lemma 3.1 motivates putting _**A**_ into a canonical form by conjugation [2], which is ideally more structured
and allows faster computation. For example, if _**A**_ were diagonal, the resulting computations become much
more tractable. In particular, the desired _**K**_ (equation (4)) would be a **Vandermonde** **product** which
theoretically only needs _O_ (( _N_ + _L_ ) log [2] ( _N_ + _L_ )) arithmetic operations [29].


Unfortunately, the naive application of diagonalization does not work due to numerical issues. Werive the
explicit diagonalization for the HiPPO matrix (2) and show it has entries exponentially large in the state size
_N_, rendering the diagonalization numerically infeasible (e.g. _**CV**_ in Lemma 3.1 would not be computable).
We note that Gu et al. [18] proposed a different (unimplemented) algorithm to compute _**K**_ faster than the
naive algorithm. In Appendix B, we prove that it is also numerically unstable for related reasons.

**Lemma** **3.2.** _The_ _HiPPO_ _matrix_ _**A**_ _in_ _equation_ (2) _is_ _diagonalized_ _by_ _the_ _matrix_ _**V**_ _ij_ = - _ii−_ + _jj_ - _._ _In_ _particular,_
_**V**_ 3 _i,i_ = �42 _ii_ - _≈_ 2 [4] _[i]_ _._ _Therefore_ _**V**_ _has_ _entries_ _of_ _magnitude_ _up_ _to_ 2 [4] _[N/]_ [3] _._


**3.2** **The** **S4** **Parameterization:** **Normal** **Plus** **Low-Rank**


The previous discussion implies that we should only conjugate by well-conditioned matrices _**V**_ . The ideal
scenario is when the matrix _**A**_ is diagonalizable by a perfectly conditioned (i.e., unitary) matrix. By the
Spectral Theorem of linear algebra, this is exactly the class of **normal** **matrices** . However, this class of
matrices is restrictive; in particular, it does not contain the HiPPO matrix (2).


We make the observation that although the HiPPO matrix is not normal, it can be decomposed as the _sum_ _of_
_a_ _normal_ _and_ _low-rank_ _matrix_ . However, this is still not useful by itself: unlike a diagonal matrix, powering
up this sum (in (5)) is still slow and not easily optimized. We overcome this bottleneck by simultaneously
applying three new techniques.


2Note that although we ultimately require _**A**_, conjugation commutes with discretization so we refer to _**A**_ .


5


**Algorithm** **1** S4 Convolution Kernel (Sketch)



**Input:** S4 parameters **Λ** _,_ _**P**_ _,_ _**Q**_ _,_ _**B**_ _,_ _**C**_ _∈_ C _[N]_ and step size ∆
**Output:** SSM convolution kernel _**K**_ = _KL_ ( _**A**_ _,_ _**B**_ _,_ _**C**_ ) for _**A**_ = **Λ** _−_ _**P Q**_ _[∗]_ (equation (5))




   - ~~_L_~~ [�] _[∗]_
1: _**C**_ _←_ _**I**_ _−_ _**A**_

[�]



_**C**_ _▷_ Truncate SSM generating function (SSMGF) to length _L_



2:




- _kk_ 0010(( _ωω_ )) _kk_ 0111(( _ωω_ ))� _←_ - _**C**_ - _**Q**_ - _∗_ - ∆2 11+ _−ωω_ _[−]_ **[Λ]** - _−_ 1 [ _**B**_ _**P**_ ] _▷_ Black-box Cauchy kernel



3: _**K**_ **[ˆ]** ( _ω_ ) _←_ 1+2 _ω_ - _k_ 00( _ω_ ) _−_ _k_ 01( _ω_ )(1 + _k_ 11( _ω_ )) _[−]_ [1] _k_ 10( _ω_ )� _▷_ Woodbury Identity



4: _**K**_ **[ˆ]** = _{_ _**K**_ **[ˆ]** ( _ω_ ) : _ω_ = exp(2 _πi_ _[k]_



_L_ [)] _[}]_ _▷_ Evaluate SSMGF at all roots of unity _ω_ _∈_ Ω _L_



5: _**K**_ _←_ iFFT( _**K**_ **[ˆ]** ) _▷_ Inverse Fourier Transform




- Instead of computing _**K**_ directly, we compute its spectrum by evaluating its **truncated** **generating**
**function** [�] _j_ _[L]_ =0 _[−]_ [1] _**[K]**_ _[j][ζ]_ _[j]_ [at] [the] [roots] [of] [unity] _[ζ]_ [.] _**[K]**_ [can] [then] [be] [found] [by] [applying] [an] [inverse] [FFT.]


- This generating function is closely related to the matrix resolvent, and now involves a matrix _inverse_
instead of _power_ . The low-rank term can now be corrected by applying the **Woodbury** **identity** which
reduces ( _**A**_ + _**P Q**_ _[∗]_ ) _[−]_ [1] in terms of _**A**_ _[−]_ [1], truly reducing to the diagonal case.


- Finally, we show that the diagonal matrix case is equivalent to the computation of a **Cauchy** **kernel**
_ωj_ _−_ 1 _ζk_ [,] [a] [well-studied] [problem] [with] [stable] [near-linear] [algorithms] [[30,] [31].]


Our techniques apply to any matrix that can be decomposed as _**Normal**_ _**Plus**_ _**Low-Rank**_ _**(NPLR)**_ .


**Theorem** **1.** _All_ _HiPPO_ _matrices_ _from_ _[16]_ _have_ _a_ _NPLR_ _representation_


_**A**_ = _**V**_ **Λ** _**V**_ _[∗]_ _−_ _**P Q**_ _[⊤]_ = _**V**_ ( **Λ** _−_ ( _**V**_ _[∗]_ _**P**_ ) ( _**V**_ _[∗]_ _**Q**_ ) _[∗]_ ) _**V**_ _[∗]_ (6)


_for_ _unitary_ _**V**_ _∈_ C _[N]_ _[×][N]_ _,_ _diagonal_ **Λ** _,_ _and_ _low-rank_ _factorization_ _**P**_ _,_ _**Q**_ _∈_ R _[N]_ _[×][r]_ _._ _These_ _matrices_ _HiPPO-_ _LegS,_
_LegT,_ _LagT_ _all_ _satisfy_ _r_ = 1 _or_ _r_ = 2 _._ _In_ _particular,_ _equation_ (2) _is_ _NPLR_ _with_ _r_ = 1 _._


**3.3** **S4** **Algorithms** **and** **Computational** **Complexity**


By equation (6), note that NPLR matrices can be conjugated into _diagonal_ _plus_ _low-rank_ (DPLR) form (now
over C instead of R ). Theorems 2 and 3 describe the complexities of SSMs where _**A**_ is in DPLR form. S4 is
optimal or near-optimal for both recurrent and convolutional representations.


**Theorem** **2** (S4 Recurrence) **.** _Given_ _any_ _step_ _size_ ∆ _,_ _computing_ _one_ _step_ _of_ _the_ _recurrence_ (3) _can_ _be_ _done_
_in_ _O_ ( _N_ ) _operations_ _where_ _N_ _is_ _the_ _state_ _size._


Theorem 2 follows from the fact that the inverse of a DPLR matrix is also DPLR (e.g. also by the Woodbury
identity). This implies that the discretized matrix _**A**_ is the product of two DPLR matrices and thus has
_O_ ( _N_ ) matrix-vector multiplication. Appendix C.2 computes _**A**_ in closed DPLR form.


**Theorem** **3** (S4 Convolution) **.** _Given_ _any_ _step_ _size_ ∆ _,_ _computing_ _the_ _SSM_ _convolution_ _filter_ _**K**_ _can_ _be_
_reduced_ _to_ _4_ _Cauchy_ _multiplies,_ _requiring_ _only_ _O_ ( _N_ + _L_ ) _operations_ _and_ _O_ ( _N_ + _L_ ) _space._

[�]


Appendix C, Definition 3 formally defines Cauchy matrices, which are related to rational interpolation
problems. Computing with Cauchy matrices is an extremely well-studied problem in numerical analysis,
with both fast arithmetic and numerical algorithms based on the famous Fast Multipole Method (FMM)

[29, 30, 31]. The computational complexities of these algorithms under various settings are described in
Appendix C, Proposition 5.


We reiterate that Theorem 3 is our core technical contribution, and its algorithm is the very motivation of
the NPLR S4 parameterization. This algorithm is formally sketched in Algorithm 1.


6


Table 1: Complexity of various sequence models in terms of sequence length ( _**L**_ ), batch size ( _**B**_ ), and hidden dimension
( _**H**_ ); tildes denote log factors. Metrics are parameter count, training computation, training space requirement, training
parallelizability, and inference computation (for 1 sample and time-step). For simplicity, the state size _N_ of S4 is tied
to _H_ . Bold denotes model is theoretically best for that metric. Convolutions are efficient for training while recurrence
is efficient for inference, while SSMs combine the strengths of both.


Convolution [3] Recurrence Attention S4


Parameters _LH_ _**H**_ **[2]** _**H**_ **[2]** _**H**_ **[2]**

Training _**LH**_ **˜** **(** _**B**_ **+** _**H**_ **)** _BLH_ [2] _B_ ( _L_ [2] _H_ + _LH_ [2] ) _**BH**_ **(** _**H**_ **[˜]** **+** _**L**_ **[˜]** **) +** _**BLH**_ **[˜]**
Space _**BLH**_ _**BLH**_ _B_ ( _L_ [2] + _HL_ ) _**BLH**_
Parallel **Yes** No **Yes** **Yes**
Inference _LH_ [2] _**H**_ **[2]** _L_ [2] _H_ + _H_ [2] _L_ _**H**_ **[2]**


**3.4** **Architecture** **Details** **of** **the** **Deep** **S4** **Layer**


Concretely, an S4 layer is parameterized as follows. First initialize a SSM with _**A**_ set to the HiPPO matrix
(2). By Lemma 3.1 and Theorem 1, this SSM is unitarily equivalent to some ( **Λ** _−_ _**P Q**_ _[∗]_ _,_ _**B**_ _,_ _**C**_ ) for some
diagonal **Λ** and vectors _**P**_ _,_ _**Q**_ _,_ _**B**_ _,_ _**C**_ _∈_ C _[N]_ _[×]_ [1] . These comprise S4’s 5 _N_ trainable parameters.


The overall deep neural network (DNN) architecture of S4 is similar to prior work. As defined above, S4
defines a map from R _[L]_ _→_ R _[L]_, i.e. a 1-D sequence map. Typically, DNNs operate on feature maps of size _H_
instead of 1. S4 handles multiple features by simply defining _H_ independent copies of itself, and then mixing
the _H_ features with a position-wise linear layer for a total of _O_ ( _H_ [2] )+ _O_ ( _HN_ ) parameters per layer. Nonlinear
activation functions are also inserted between these layers. Overall, S4 defines a sequence-to-sequence map of
shape (batch size, sequence length, hidden dimension), exactly the same as related sequence models such as
Transformers, RNNs, and CNNs.


Note that the core S4 module is a linear transformation, but the addition of non-linear transformations
through the depth of the network makes the overall deep SSM non-linear. This is analogous to a vanilla CNN,
since convolutional layers are also linear. The broadcasting across _H_ hidden features described in this section
is also analogous to depthwise-separable convolutions. Thus, the overall deep S4 model is closely related to a
depthwise-separable CNN but with _global_ convolution kernels.


Finally, we note that follow-up work found that this version of S4 can sometimes suffer from numerical
instabilities when the _**A**_ matrix has eigenvalues on the right half-plane [14]. It introduced a slight change to
the NPLR parameterization for S4 from **Λ** _−_ _**P Q**_ _[∗]_ to **Λ** _−_ _**P P**_ _[∗]_ that corrects this potential problem.


Table 1 compares the complexities of the most common deep sequence modeling mechanisms.

##### **4 Experiments**


Section 4.1 benchmarks S4 against the LSSL and efficient Transformer models. Section 4.2 validates S4 on
LRDs: the LRA benchmark and raw speech classification. Section 4.3 investigates whether S4 can be used as
a general sequence model to perform effectively and efficiently in a wide variety of settings including image
classification, image and text generation, and time series forecasting.


**4.1** **S4** **Efficiency** **Benchmarks**


We benchmark that S4 can be trained quickly and efficiently, both compared to the LSSL, as well as efficient
Transformer variants designed for long-range sequence modeling. As outlined in Section 3, S4 is theoretically
much more efficient than the LSSL, and Table 2 confirms that the S4 is orders of magnitude more speed- and
memory-efficient for practical layer sizes. In fact, S4’s speed and memory use is competitive with the most


3Refers to global (in the sequence length) and depthwise-separable convolutions, similar to the convolution version of S4.


7


Table 2: Deep SSMs: The S4 parameterization with Algorithm 1
is asymptotically more efficient than the LSSL.


Training Step (ms) Memory Alloc. (MB)


Dim. 128 256 512 128 256 512


LSSL 9.32 20.6 140.7 222.1 1685 13140
**S4** 4.77 3.07 4.75 5.3 12.6 33.5


Ratio 1 _._ 9 _×_ 6 _._ 7 _×_ **29** _**.**_ **6** _×_ 42 _._ 0 _×_ 133 _×_ **392** _**×**_



Table 3: Benchmarks vs. efficient Transformers


Length 1024 Length 4096


Speed Mem. Speed Mem.


Transformer 1 _×_ 1 _×_ 1 _×_ 1 _×_


Performer 1.23 _×_ 0.43 _×_ 3.79 _×_ 0.086 _×_
Linear Trans. **1.58** _×_ **0.37** _×_ **5.35** _×_ **0.067** _×_


**S4** **1.58** _×_ 0.43 _×_ 5.19 _×_ 0.091 _×_



Figure 2: Visualizations of a trained S4 model on LRA Path-X. SSM convolution kernels _**K**_ _∈_ R [16384] are reshaped
into a 128 _×_ 128 image. ( _Left_ ) Example from the Path-X task, which involves deducing if the markers are connected
by a path ( _Top_ ) Filters from the first layer ( _Bottom_ ) Filters from the last layer.


Table 4: ( **Long** **Range** **Arena** ) ( _Top_ ) Original Transformer variants in LRA. Full results in Appendix D.2. ( _Bottom_ )
Other models reported in the literature. _Please_ _read_ _Appendix_ _D.5_ _before_ _citing_ _this_ _table._


Model ListOps Text Retrieval Image Pathfinder Path-X Avg


Transformer 36.37 64.27 57.46 42.44 71.40 53.66
Reformer 37.27 56.10 53.40 38.07 68.50 50.56
BigBird 36.05 64.02 59.29 40.83 74.87 54.17
Linear Trans. 16.13 65.90 53.09 42.34 75.30 50.46
Performer 18.01 65.40 53.82 42.77 77.05 51.18


FNet 35.33 65.11 59.61 38.67 77.80 54.42
Nystr¨omformer 37.15 65.52 79.56 41.58 70.94 57.46
Luna-256 37.25 64.57 79.29 47.38 77.72 59.37
**S4** **59.60** **86.82** **90.90** **88.65** **94.20** **96.35** **86.09**


efficient Transformer variants benchmarked by Tay et al. [40]—Linear Transformer [22] and Performer [8]—in
a parameter-matched setting (Table 3, following the protocol of Tay et al. [40]).


**4.2** **Learning** **Long** **Range** **Dependencies**


As described in Sections 2.2 and 3.1, S4 uses a principled approach to address LRDs based on the HiPPO
theory of continuous-time memorization. Our goal in this section is to validate that S4 achieves high
performance on difficult tasks that require long-range reasoning. We focus here on two problems: (i) the
Long-Range Arena, a well-known benchmark designed to test efficient sequence models on LRDs, and (ii) a
speech classification problem as a real-world test of LRDs.


**Long** **Range** **Arena** **(LRA).** LRA [40] contains 6 tasks with lengths 1K-16K steps, encompassing modalities


8


and objectives that require similarity, structural, and visuospatial reasoning. Table 4 compares S4 against
the 11 Transformer variants from Tay et al. [40] as well as follow-up work. S4 substantially advances the
SoTA, outperforming all baselines on all tasks and averaging 80 _._ 48% compared to less than 60% for every
baseline. Notably, S4 solves the Path-X task, an extremely challenging task that involves reasoning about
LRDs over sequences of length 128 _×_ 128 = 16384. All previous models have failed (i.e. random guessing) due
to memory or computation bottlenecks, or simply being unable to learn such long dependencies.


We analyze S4’s performance on Path-X by visualizing its learned representations, in particular 1-D convolution
kernels _**K**_ which are the focus of our technical results in Section 3. Fig. 2 shows that S4 learns a variety of
filters that display spatially consistent structure and demonstrate awareness of the 2-D nature of the data. In
particular, the lower layers learn simple kernels that extract features from just a few rows of local context
while ignoring the rest of the image. On the other hand, higher layers aggregate information globally across
full columns of the image at varying spatial frequencies. Filters in these higher layers span the entire context
(16384 pixels), confirming S4’s ability to learn LRDs.


**Raw** **Speech** **Classification.** Speech is a typical real-world time series domain, involving signals sampled
from an underlying physical process at high frequency. We perform speech classification using the SC10 subset
of the _Speech_ _Commands_ dataset [47] (see Appendix D.5). While most sequence models for speech rely on
extensive preprocessing (e.g. to MFCC features), we classify raw speech (length-16000) following Romero et al.

[35]. S4 achieves 98 _._ 3% accuracy, higher than all baselines that use the 100 _×_ shorter MFCC features, and
validates that a powerful LRD model is able to extract more information from the raw data and outperform
hand-crafted pre-processing. Additionally, we include a baseline CNN specifically designed for raw speech,
the discriminator from the WaveGAN model [11], which performs worse than S4 while having 90 _×_ more
parameters and incorporating many more architectural heuristics (Appendix D.2).


**4.3** **S4** **as** **a** **General** **Sequence** **Model**


A key goal of sequence modeling research is to develop a single model that can be applied in many domains
(e.g. images, audio, text, time-series) with a broad range of capabilities (e.g. efficient training, fast generation,
handling irregularly sampled data). As a fundamental scientific model, SSMs are a promising candidate that
come with a range of capabilities, and S4’s strong results on LRD benchmarks spanning images, text, and
speech are evidence of S4’s potential as a general sequence model. In this section, we focus on understanding
this question in more depth by highlighting key strengths of S4 in settings that usually require specialized



Table 5: ( **SC10** **classification** ) Transformer, CTM,
RNN, CNN, and SSM models. ( _MFCC_ ) Standard preprocessed MFCC features (length 161). ( _Raw_ ) Unprocessed signals (length 16000). ( _0_ _.5_ _×_ ) Frequency change
at test time. denotes not applicable or computationally infeasible on single GPU. _Please_ _read_ _Appendix_ _D.5_
_before_ _citing_ _this_ _table._


MFCC Raw 0 _._ 5 _×_


Transformer 90.75
Performer 80.85 30.77 30.68


ODE-RNN 65.9
NRDE 89.8 16.49 15.12


ExpRNN 82.13 11.6 10.8
LipschitzRNN 88.38


CKConv **95.3** 71.66 65.96
WaveGAN-D 96.25


LSSL 93.58
**S4** 93.96 **98.32** **96.30**



Table 6: ( **Pixel-level** **1-D** **image** **classification** )
Comparison against reported test accuracies from prior
works (Transformer, RNN, CNN, and SSM models).
Extended results and citations in Appendix D.


sMNIST pMNIST sCIFAR


Transformer 98.9 97.9 62.2


LSTM 98.9 95.11 63.01
r-LSTM 98.4 95.2 72.2
UR-LSTM 99.28 96.96 71.00
UR-GRU 99.27 96.51 74.4
HiPPO-RNN 98.9 98.3 61.1
LMU-FFT - 98.49 LipschitzRNN 99.4 96.3 64.2


TCN 99.0 97.2 TrellisNet 99.20 98.13 73.42
CKConv 99.32 98.54 63.74


LSSL 99.53 **98.76** 84.65
**S4** **99.63** 98.70 **91.13**



9


Table 7: ( **CIFAR-10** **density** **estimation** ) As a generic
sequence model, S4 is competitive with previous autoregressive
models (in bits per dim.) while incorporating no 2D inductive
bias, and has fast generation through its recurrence mode.


Model bpd 2D bias Images / sec


Transformer 3.47 **None** 0.32 (1 _×_ )
Linear Transf. 3.40 **None** 17.85 (56 _×_ )
PixelCNN 3.14 2D conv. Row PixelRNN 3.00 2D BiLSTM PixelCNN++ 2.92 2D conv. 19.19 (59 _._ 97 _×_ )
Image Transf. 2.90 2D local attn. 0.54 (1.7 _×_ )
PixelSNAIL 2.85 2D conv. + attn. 0.13 (0.4 _×_ )
Sparse Transf. **2.80** 2D sparse attn. 

**S4** (base) 2.92 **None** **20.84** ( **65** _**.**_ **1** _**×**_ )
**S4** (large) 2.85 **None** 3.36 (10 _._ 5 _×_ )



Table 8: ( **WikiText-103** **language** **modeling** ) S4 approaches the performance of Transformers with much
faster generation. ( _Top_ ) Transformer baseline which our
implementation is based on, with attention replaced by
S4. ( _Bottom_ ) Attention-free models (RNNs and CNNs).


Model Params Test ppl. Tokens / sec


Transformer 247M **20.51** 0.8K (1 _×_ )


GLU CNN 229M 37.2 AWD-QRNN 151M 33.0 LSTM + Hebb. - 29.2 TrellisNet 180M 29.19 Dynamic Conv. 255M 25.0 TaLK Conv. 240M 23.3 **S4** 249M **20.95** **48K** ( **60** _**×**_ )



models. The tasks we focus on (generative modeling, image classification, time-series forecasting) are
considered as LRD tasks in the literature, and serve as additional validation that S4 handles LRDs efficiently.


**Large-scale** **generative** **modeling.** We investigate two well-studied image and text benchmarks to validate
the scalability, flexibility, and efficiency of S4. These tasks require much larger models than our previous
tasks - up to 250M parameters.


First, CIFAR density estimation is a popular benchmark for autoregressive models, where images are flattened
into a sequence of 3072 RGB subpixels that are predicted one by one. Table 7 shows that _with_ _no_ _2D_ _inductive_
_bias_, S4 is competitive with the best models designed for this task.


Second, WikiText-103 is an established benchmark for language modeling, an important task for large-scale
sequence models where tokens are predicted sequentially based on past context. Although RNNs were the
model of choice for many years, Transformers are now the dominant model in such applications that contain
data that is inherently discrete. We show that alternative models to Transformers can still be competitive in
these settings. By simply taking a strong Transformer baseline [2] and replacing the self-attention layers, S4
substantially closes the gap to Transformers (within 0 _._ 8 ppl), setting SoTA for attention-free models by over
2 ppl.


**Fast** **autoregressive** **inference.** A prominent limitation of autoregressive models is inference speed (e.g.
generation), since they require a pass over the full context for every new sample. Several methods have been
specifically crafted to overcome this limitation such as the Linear Transformer, a hybrid Transformer/RNN
that switches to a stateful, recurrent view at inference time for speed.


As a stateful model, SSMs automatically have this ability (Fig. 1). By switching to its recurrent representation
(Section 2.3), S4 requires _constant_ _memory_ _and_ _computation_ per time step - in contrast to standard
autoregressive models which scale in the context length. On both CIFAR-10 and WikiText-103, we report
the throughput of various models at generation time, with S4 around 60 _×_ faster than a vanilla Transformer
on both tasks (details in Appendix D.3.3).


**Sampling** **resolution** **change.** As a continuous-time model, S4 automatically adapts to data sampled
at different rates, a challenging setting for time series with a dedicated line of work [10, 35, 37]. Without
re-training, S4 achieves 96 _._ 3% accuracy at 0 _._ 5 _×_ the frequency on Speech Commands 10 (Table 5), simply by
changing its internal step size ∆(Section 2.3).


**Learning** **with** **weaker** **inductive** **bias.** Beyond our results on speech (Section 4.2), we further validate
that S4 can be applied with minimal modifications on two domains that typically require specialized domainspecific preprocessing and architectures. First, we compare S4 to the Informer [50], a new Transformer
architecture that uses a complex encoder-decoder designed for time-series forecasting problems. A simple
application of S4 that treats forecasting as a masked sequence-to-sequence transformation (Fig. 5) outperforms
the Informer and other baselines on 40 _/_ 50 settings across 5 forecasting tasks. Notably, S4 is better on the


10


longest setting in each task, e.g. reducing MSE by 37% when forecasting 30 days of weather data (Table 9).


Finally, we evaluate S4 on pixel-level sequential image classification tasks (Table 6), popular benchmarks
which were originally LRD tests for RNNs [1]. Beyond LRDs, these benchmarks point to a recent effort of
the ML community to solve vision problems with reduced domain knowledge, in the spirit of models such as
Vision Transformers [12] and MLP-Mixer [41] which involve patch-based models that without 2-D inductive
bias. Sequential CIFAR is a particularly challenging dataset where outside of SSMs, all sequence models have
a gap of over 25% to a simple 2-D CNN. By contrast, S4 is competitive with a larger ResNet18 (7.9M vs.
11.0M parameters), both with ( **93.16%** vs. 95.62%) or without ( **91.12%** vs. 89.46%) data augmentation.
Moreover, it is much more robust to other architectural choices (e.g. **90.46%** vs. 79.52% when swapping
BatchNorm for LayerNorm).


**4.4** **SSM** **Ablations:** **the** **Importance** **of** **HiPPO**


A critical motivation of S4 was the use of the HiPPO matrices to initialize an SSM. We consider several
simplifications of S4 to ablate the importance of each of these components, including: (i) how important is
the HiPPO initialization? (ii) how important is training the SSM on top of HiPPO? (iii) are the benefits of
S4 captured by the NPLR parameterization without HiPPO?


As a simple testbed, all experiments in this section were performed on the sequential CIFAR-10 task, whicih
we found transferred well to other settings. Models were constrained to at most 100K trainable parameters
and trained with a simple plateau learning rate scheduler and no regularization.


**Unconstrained** **SSMs.** We first investigate generic SSMs with various initializations. We consider a
random Gaussian initialization (with variance scaled down until it did not NaN), and the HiPPO initialization.
We also consider a random diagonal Gaussian matrix as a potential structured method; parameterizing _**A**_ as
a diagonal matrix would allow substantial speedups without going through the complexity of S4’s NPLR
parameterization. We consider both freezing the _**A**_ matrix and training it.


Fig. 3 shows both training and validation curves, from which we can make several observations. First, training
the SSM improved all methods, particularly the randomly initialized ones. For all methods, training the SSM
led to improvements in both training and validation curves.


Second, a large generalization gap exists between the initializations. In particular, note that when _**A**_ is
trained, all initializations are able to reach perfect training accuracy. However, their validation accuracies are
separated by over 15%.


**NPLR** **SSMs.** The previous experiment validates the importance of HiPPO in SSMs. This was the main
motivation of the NPLR algorithm in S4, which utilizes structure of the HiPPO matrix (2) to make SSMs
computationally feasible. Fig. 4a shows that random NPLR matrices still do not perform well, which validates
that S4’s effectiveness primarily comes from the HiPPO initialization, not the NPLR parameterization.


Finally, Fig. 4b considers the main ablations considered in this section (with trainable SSMs) and adds minor
regularization. With 0.1 Dropout, the same trends still hold, and the HiPPO initialization—in other words,
the full S4 method—achieves 84 _._ 27% test accuracy with just 100K parameters.


Table 9: Univariate long sequence time-series forecasting results. Full results in Appendix D.3.5.


**S4** Informer LogTrans Reformer LSTMa DeepAR ARIMA Prophet


MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE


ETTh1 **0.116** **0.271** 0.269 0.435 0.273 0.463 2.112 1.436 0.683 0.768 0.658 0.707 0.659 0.766 2.735 3.253
ETTh2 **0.187** **0.358** 0.277 0.431 0.303 0.493 2.030 1.721 0.640 0.681 0.429 0.580 2.878 1.044 3.355 4.664
ETTm1 **0.292** **0.466** 0.512 0.644 0.598 0.702 1.793 1.528 1.064 0.873 2.437 1.352 0.639 0.697 2.747 1.174
Weather **0.245** **0.375** 0.359 0.466 0.388 0.499 2.087 1.534 0.866 0.809 0.499 0.596 1.062 0.943 3.859 1.144
ECL **0.432** **0.497** 0.582 0.608 0.624 0.645 7.019 5.105 1.545 1.006 0.657 0.683 1.370 0.982 6.901 4.264


11


Figure 3: CIFAR-10 classification with unconstrained, real-valued SSMs with various initializations. ( _Left_ ) Train
accuracy. ( _Right_ ) Validation accuracy.


(a) (b)


Figure 4: CIFAR-10 validation accuracy of SSMs with different initializations and parameterizations. ( _Left_ ) NPLR
parameterization with random versus HiPPO initialization. ( _Right_ ) All methods considered in this section, including
minor Dropout regularization. S4 achieves SotA accuracy on sequential CIFAR-10 with just 100K parameters.

##### **5 Conclusion**


We introduce S4, a sequence model that uses a new parameterization for the state space model’s continuoustime, recurrent, and convolutional views to efficiently model LRDs in a principled manner. Results across
established benchmarks evaluating a diverse range of data modalities and model capabilities suggest that S4
has the potential to be an effective general sequence modeling solution.


**Acknowledgments**


We thank Aditya Grover and Chris Cundy for helpful discussions about earlier versions of the method. We
thank Simran Arora, Sabri Eyuboglu, Bibek Paudel, and Nimit Sohoni for valuable feedback on earlier drafts
of this work. This work was done with the support of Google Cloud credits under HAI proposals 540994170283
and 578192719349. We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize),
NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML);
ONR under No. N000141712266 (Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding
and Applying Non-Euclidean Geometry in Machine Learning; N000142012275 (NEPTUNE); the Moore
Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF,
Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google
Cloud, Salesforce, Total, the HAI-AWS Cloud Credits for Research program, the Stanford Data Science
Initiative (SDSI), and members of the Stanford DAWN project: Facebook, Google, and VMWare. The
Mobilize Center is a Biomedical Technology Resource Center, funded by the NIH National Institute of
Biomedical Imaging and Bioengineering through Grant P41EB027060. The U.S. Government is authorized
to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation
thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of


12


the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of
NIH, ONR, or the U.S. Government.

##### **References**


[1] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In _The_
_International_ _Conference_ _on_ _Machine_ _Learning_ _(ICML)_, pages 1120–1128, 2016.


[2] Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. _arXiv_
_preprint_ _arXiv:1809.10853_, 2018.


[3] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and
recurrent networks for sequence modeling. _arXiv_ _preprint_ _arXiv:1803.01271_, 2018.


[4] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Trellis networks for sequence modeling. In _The_
_International_ _Conference_ _on_ _Learning_ _Representations_ _(ICLR)_, 2019.


[5] Shiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael Witbrock,
Mark Hasegawa-Johnson, and Thomas S Huang. Dilated recurrent neural networks. In _Advances_ _in_
_Neural_ _Information_ _Processing_ _Systems_ _(NeurIPS)_, 2017.


[6] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. _arXiv_ _preprint_ _arXiv:1904.10509_, 2019.


[7] Narsimha Chilkuri and Chris Eliasmith. Parallelizing legendre memory unit training. _The_ _International_
_Conference_ _on_ _Machine_ _Learning_ _(ICML)_, 2021.


[8] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas
Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with
performers. In _The_ _International_ _Conference_ _on_ _Learning_ _Representations_ _(ICLR)_, 2020.


[9] Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated
convolutional networks. In _International_ _conference_ _on_ _machine_ _learning_, pages 933–941. PMLR, 2017.


[10] Edward De Brouwer, Jaak Simm, Adam Arany, and Yves Moreau. Gru-ode-bayes: Continuous modeling
of sporadically-observed time series. In _Advances_ _in_ _Neural_ _Information_ _Processing_ _Systems_ _(NeurIPS)_,
2019.


[11] Chris Donahue, Julian McAuley, and Miller Puckette. Adversarial audio synthesis. In _ICLR_, 2019.


[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is
worth 16x16 words: Transformers for image recognition at scale. _arXiv_ _preprint_ _arXiv:2010.11929_, 2020.


[13] N Benjamin Erichson, Omri Azencot, Alejandro Queiruga, Liam Hodgkinson, and Michael W Mahoney.
Lipschitz recurrent neural networks. In _International_ _Conference_ _on_ _Learning_ _Representations_, 2021.


[14] Karan Goel, Albert Gu, Chris Donahue, and Christopher R´e. It’s raw! audio generation with state-space
models. _arXiv_ _preprint_ _arXiv:2202.09729_, 2022.


[15] Gene H Golub and Charles F Van Loan. _Matrix_ _computations_, volume 3. JHU press, 2013.


[16] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R´e. Hippo: Recurrent memory with
optimal polynomial projections. In _Advances_ _in_ _Neural_ _Information_ _Processing_ _Systems_ _(NeurIPS)_, 2020.


[17] Albert Gu, Caglar Gulcehre, Tom Le Paine, Matt Hoffman, and Razvan Pascanu. Improving the gating
mechanism of recurrent neural networks. In _The_ _International_ _Conference_ _on_ _Machine_ _Learning_ _(ICML)_,
2020.


13


[18] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R´e. Combining
recurrent, convolutional, and continuous-time models with the structured learnable linear state space
layer. In _Advances_ _in_ _Neural_ _Information_ _Processing_ _Systems_ _(NeurIPS)_, 2021.


[19] Albert Gu, Ankit Gupta, Karan Goel, and Christopher R´e. On the parameterization and initialization
of diagonal state space models. _arXiv_ _preprint_ _arXiv:2206.11893_, 2022.


[20] Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher R´e. How to train your hippo:
State space models with generalized basis projections. _arXiv_ _preprint_ _arXiv:2206.12037_, 2022.


[21] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. _Neural computation_, 9(8):1735–1780,
1997.


[22] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran¸cois Fleuret. Transformers are rnns:
Fast autoregressive transformers with linear attention. In _International_ _Conference_ _on_ _Machine_ _Learning_,
pages 5156–5165. PMLR, 2020.


[23] Patrick Kidger, James Morrill, James Foster, and Terry Lyons. Neural controlled differential equations
for irregular time series. _arXiv_ _preprint_ _arXiv:2005.08926_, 2020.


[24] Mario Lezcano-Casado and David Mart´ınez-Rubio. Cheap orthogonal constraints in neural networks:
A simple parametrization of the orthogonal and unitary group. In _The_ _International_ _Conference_ _on_
_Machine_ _Learning_ _(ICML)_, 2019.


[25] Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo Gao. Independently recurrent neural network
(IndRNN): Building a longer and deeper RNN. In _Proceedings_ _of_ _the_ _IEEE_ _Conference_ _on_ _Computer_
_Vision_ _and_ _Pattern_ _Recognition_, pages 5457–5466, 2018.


[26] Vasileios Lioutas and Yuhong Guo. Time-aware large kernel convolutions. In _International_ _Conference_
_on_ _Machine_ _Learning_, pages 6172–6183. PMLR, 2020.


[27] Stephen Merity, Nitish Shirish Keskar, James Bradbury, and Richard Socher. Scalable language modeling:
Wikitext-103 on a single gpu in 12 hours. _SysML_, 2018.


[28] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal
Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio.
_arXiv_ _preprint_ _arXiv:1609.03499_, 2016.


[29] Victor Pan. _Structured_ _matrices_ _and_ _polynomials:_ _unified_ _superfast_ _algorithms_ . Springer Science &
Business Media, 2001.


[30] Victor Pan. Fast approximate computations with cauchy matrices and polynomials. _Mathematics_ _of_
_Computation_, 86(308):2799–2826, 2017.


[31] Victor Y Pan. Transformations of matrix structures work again. _Linear_ _Algebra_ _and_ _Its_ _Applications_,
465:107–138, 2015.


[32] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural
networks. In _International_ _conference_ _on_ _machine_ _learning_, pages 1310–1318, 2013.


[33] Jack Rae, Chris Dyer, Peter Dayan, and Timothy Lillicrap. Fast parametric learning with activation
memorization. _The_ _International_ _Conference_ _on_ _Machine_ _Learning_ _(ICML)_, 2018.


[34] Prajit Ramachandran, Tom Le Paine, Pooya Khorrami, Mohammad Babaeizadeh, Shiyu Chang, Yang
Zhang, Mark A Hasegawa-Johnson, Roy H Campbell, and Thomas S Huang. Fast generation for
convolutional autoregressive models. _arXiv_ _preprint_ _arXiv:1704.06001_, 2017.


[35] David W Romero, Anna Kuzina, Erik J Bekkers, Jakub M Tomczak, and Mark Hoogendoorn. Ckconv:
Continuous kernel convolution for sequential data. _arXiv_ _preprint_ _arXiv:2102.02611_, 2021.


14


[36] David W Romero, Robert-Jan Bruintjes, Jakub M Tomczak, Erik J Bekkers, Mark Hoogendoorn, and
Jan C van Gemert. Flexconv: Continuous kernel convolutions with differentiable kernel sizes. In _The_
_International_ _Conference_ _on_ _Learning_ _Representations_ _(ICLR)_, 2022.


[37] Yulia Rubanova, Tian Qi Chen, and David K Duvenaud. Latent ordinary differential equations for
irregularly-sampled time series. In _Advances_ _in_ _Neural_ _Information_ _Processing_ _Systems_, pages 5321–5331,
2019.


[38] T Konstantin Rusch and Siddhartha Mishra. Unicornn: A recurrent model for learning very long time
dependencies. _The_ _International_ _Conference_ _on_ _Machine_ _Learning_ _(ICML)_, 2021.


[39] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the pixelcnn
with discretized logistic mixture likelihood and other modifications. _arXiv_ _preprint_ _arXiv:1701.05517_,
2017.


[40] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu
Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers.
In _International_ _Conference_ _on_ _Learning_ _Representations_, 2021. URL `[https://openreview.net/forum?](https://openreview.net/forum?id=qVyeW-grC2k)`
`[id=qVyeW-grC2k](https://openreview.net/forum?id=qVyeW-grC2k)` .


[41] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner,
Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, et al. Mlp-mixer: An all-mlp architecture
for vision. _arXiv_ _preprint_ _arXiv:2105.01601_, 2021.


[42] Trieu H Trinh, Andrew M Dai, Minh-Thang Luong, and Quoc V Le. Learning longer-term dependencies
in RNNs with auxiliary losses. In _The_ _International_ _Conference_ _on_ _Machine_ _Learning_ _(ICML)_, 2018.


[43] Arnold Tustin. A method of analysing the behaviour of linear systems in terms of time series. _Journal_
_of_ _the_ _Institution_ _of_ _Electrical_ _Engineers-Part_ _IIA:_ _Automatic_ _Regulators_ _and_ _Servo_ _Mechanisms_, 94(1):
130–142, 1947.


[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances_ _in_ _Neural_ _Information_ _Processing_
_Systems_ _(NeurIPS)_, 2017.


[45] Aaron Voelker, Ivana Kaji´c, and Chris Eliasmith. Legendre memory units: Continuous-time representation
in recurrent neural networks. In _Advances_ _in_ _Neural_ _Information_ _Processing_ _Systems_, pages 15544–15553,
2019.


[46] Aaron Russell Voelker. _Dynamical_ _systems_ _in_ _spiking_ _neuromorphic_ _hardware_ . PhD thesis, University of
Waterloo, 2019.


[47] Pete Warden. Speech commands: A dataset for limited-vocabulary speech recognition. _ArXiv_,
abs/1804.03209, 2018.


[48] Max A Woodbury. Inverting modified matrices. _Memorandum_ _report_, 42:106, 1950.


[49] Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, and Michael Auli. Pay less attention with
lightweight and dynamic convolutions. In _The_ _International_ _Conference_ _on_ _Learning_ _Representations_
_(ICLR)_, 2019.


[50] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.
Informer: Beyond efficient transformer for long sequence time-series forecasting. In _The_ _Thirty-Fifth_
_AAAI_ _Conference_ _on_ _Artificial_ _Intelligence,_ _AAAI_ _2021,_ _Virtual_ _Conference_, volume 35, pages 11106–
11115. AAAI Press, 2021.


15


##### **A Discussion**

**Related** **Work.** Our work is most closely related to a line of work originally motivated by a particular
biologically-inspired SSM, which led to mathematical models for addressing LRDs. Voelker et al. [45], Voelker

[46] derived a non-trainable SSM motivated from approximating a neuromorphic spiking model, and Chilkuri
and Eliasmith [7] showed that it could be sped up at train time with a convolutional view. Gu et al. [16]
extended this special case to a general continuous-time function approximation framework with several more
special cases of _**A**_ matrices designed for long-range dependencies. However, instead of using a true SSM, all
of these works fixed a choice of _**A**_ and built RNNs around it. Most recently, Gu et al. [18] used the full (1)
explicitly as a deep SSM model, exploring new conceptual views of SSMs, as well as allowing _**A**_ to be trained.
As mentioned in Section 1, their method used a naive instantiation of SSMs that suffered from an additional
factor of _N_ in memory and _N_ [2] in computation.


Beyond this work, our technical contributions (Section 3) on the S4 parameterization and algorithms are
applicable to a broader family of SSMs including these investigated in prior works, and our techniques for
working with these models may be of independent interest.


**Implementation.** The computational core of S4’s training algorithm is the Cauchy kernel discussed in
Sections 3.2 and 3.3 and Appendix C.3. As described in Appendix C.3 Proposition 5, there are many
algorithms for it with differing computational complexities and sophistication. Our current implementation
of S4 actually uses the naive _O_ ( _NL_ ) algorithm which is easily parallelized on GPUs and has more easily
accessible libraries allowing it to be implemented; we leverage the `pykeops` library for memory-efficient
kernel operations. However, this library is a much more general library that may not be optimized for the
Cauchy kernels used here, and we believe that a dedicated CUDA implementation can be more efficient.
Additionally, as discussed in this work, there are asymptotically faster and numerically stable algorithms for
the Cauchy kernel (Proposition 5). However, these algorithms are currently not implemented for GPUs due
to a lack of previous applications that require them. We believe that more efficient implementations of these
self-contained computational kernels are possible, and that S4 (and SSMs at large) may have significant room
for further improvements in efficiency.


**Limitations** **and** **Future** **Directions.** In this work, we show that S4 can address a wide variety of data
effectively. However, it may not necessarily be the most suitable model for all types of data. For example,
Table 8 still found a gap compared to Transformers for language modeling. An interesting future direction is
exploring combinations of S4 with other sequence models to complement their strengths. We are excited
about other directions, including continuing to explore the benefits of S4 on audio data (e.g. pre-training
or generation settings), and generalizing HiPPO and S4 to higher-dimensional data for image and video
applications.

##### **B Numerical Instability of LSSL**


This section proves the claims made in Section 3.1 about prior work. We first derive the explicit diagonalization
of the HiPPO matrix, confirming its instability because of exponentially large entries. We then discuss the
proposed theoretically fast algorithm from [18] (Theorem 2) and show that it also involves exponentially large
terms and thus cannot be implemented.


16


**B.1** **HiPPO** **Diagonalization**


_Proof_ _of_ _Lemma_ _3.2._ The HiPPO matrix (2) is equal, up to sign and conjugation by a diagonal matrix, to













_**A**_ =


_**A**_ _nk_ =







1

_−_ 1 2
1 _−_ 3 3

_−_ 1 3 _−_ 5 4
1 _−_ 3 5 _−_ 7 5

_−_ 1 3 _−_ 5 7 _−_ 9 6
1 _−_ 3 5 _−_ 7 9 _−_ 11 7

_−_ 1 3 _−_ 5 7 _−_ 9 11 _−_ 13 8
... ...



( _−_ 1) _[n][−][k]_ (2 _k_ + 1) _n > k_

_k_ + 1 _n_ = _k_ _._

0 _n < k_












Our goal is to show that this _**A**_ is diagonalized by the matrix














  - _i_ + _j_
_**V**_ =
_i −_ _j_



=

_ij_











1
1 1
1 3 1
1 6 5 1
1 10 15 7 1
1 15 35 28 9 1
... ...



_,_



or in other words that columns of this matrix are eigenvectors of _**A**_ .


Concretely, we will show that the _j_ -th column of this matrix _**v**_ [(] _[j]_ [)] with elements



_**v**_ _i_ [(] _[j]_ [)] =




0 _i < j_

 - _ii−_ + _jj_  - =  - _i_ 2+ _jj_  - _i ≥_ _j_



is an eigenvector with eigenvalue _j_ + 1. In other words we must show that for all indices _k_ _∈_ [ _N_ ],


( _**Av**_ [(] _[j]_ [)] ) _k_ =                    - _**A**_ _ki_ _**v**_ _i_ [(] _[j]_ [)] = ( _j_ + 1) _**v**_ _k_ [(] _[j]_ [)] _[.]_ (7)

_i_


If _k_ _< j_, then for all _i_ inside the sum, either _k_ _< i_ or _i < j_ . In the first case _**A**_ _ki_ = 0 and in the second case
_**v**_ _i_ [(] _[j]_ [)] = 0, so both sides of equation (7) are equal to 0.


It remains to show the case _k_ _≥_ _j_, which proceeds by induction on _k_ . Expanding equation (7) using the
formula for _**A**_ yields




_._



( _**Av**_ ) [(] _k_ _[j]_ [)] = - _**A**_ _ki_ _**v**_ _i_ [(] _[j]_ [)] =

_i_



_k_ - _−_ 1( _−_ 1) _[k][−][i]_ (2 _i_ + 1)� _i_ + _j_

2 _j_
_i_ = _j_




- - _k_ + _j_
+ ( _k_ + 1)
2 _j_



In the base case _k_ = _j_, the sum disappears and we are left with ( _**Av**_ [(] _[j]_ [)] ) _j_ = ( _j_ + 1)�22 _jj_ - = ( _j_ + 1) _**v**_ _j_ [(] _[j]_ [)][,] [as]
desired.

Otherwise, the sum for ( _**Av**_ ) [(] _k_ _[j]_ [)] is the same as the sum for ( _**Av**_ ) [(] _k_ _[j]_ _−_ [)] 1 [but] [with] [sign] [reversed] [and] [a] [few] [edge]


17


terms. The result follows from applying the inductive hypothesis and algebraic simplification:




          - _k −_ 1 + _j_
( _**Av**_ ) [(] _k_ _[j]_ [)] = _−_ ( _**Av**_ ) [(] _k_ _[j]_ _−_ [)] 1 _[−]_ [(2] _[k][ −]_ [1)]
2 _j_




- - _k −_ 1 + _j_
+ _k_
2 _j_




- - _k_ + _j_
+ ( _k_ + 1)
2 _j_












    - _k −_ 1 + _j_
= _−_ ( _j_ + 1)
2 _j_

    - _k −_ 1 + _j_
= _−_ ( _j_ + _k_ )
2 _j_




- - _k −_ 1 + _j_ - - _k_ + _j_

_−_ ( _k −_ 1) + ( _k_ + 1)
2 _j_ 2 _j_




- - _k_ + _j_ + ( _k_ + 1)
2 _j_



( _k −_ 1 + _j_ )!        - _k_ + _j_        = _−_ ( _j_ + _k_ )
( _k −_ 1 _−_ _j_ )!(2 _j_ )! [+ (] _[k]_ [ + 1)] 2 _j_



( _k_ + _j_ )!      - _k_ + _j_
= _−_
( _k −_ 1 _−_ _j_ )!(2 _j_ )! [+ (] _[k]_ [ + 1)] 2 _j_







( _k_ + _j_ )!        - _k_ + _j_        = _−_ ( _k −_ _j_ )
( _k −_ _j_ )!(2 _j_ )! [+ (] _[k]_ [ + 1)] 2 _j_




     - _k_ + _j_
= ( _j −_ _k_ )( _k_ + 1)
2 _j_

= ( _j_ + 1) _**v**_ _k_ [(] _[j]_ [)] _[.]_




- - _k_ + _j_
+ ( _k_ + 1)
2 _j_







**B.2** **Fast** **but** **Unstable** **LSSL** **Algorithm**


Instead of diagonalization, Gu et al. [18, Theorem 2] proposed a sophisticated fast algorithm to compute


~~_L_~~ _−_ 1
_KL_ ( _**A**_ _,_ _**B**_ _,_ _**C**_ ) = ( _**CB**_ _,_ _**CAB**_ _, . . .,_ _**CA**_ _**B**_ ) _._


This algorithm runs in _O_ ( _N_ log [2] _N_ + _L_ log _L_ ) operations and _O_ ( _N_ + _L_ ) space. However, we now show that
this algorithm is also numerically unstable.


There are several reasons for the instability of this algorithm, but most directly we can pinpoint a particular
intermediate quantity that they use.


**Definition** **1.** _The_ _fast_ _LSSL_ _algorithm_ _computes_ _coefficients_ _of_ _p_ ( _x_ ) _,_ _the_ _characteristic_ _polynomial_ _of_ _A,_ _as_
_an_ _intermediate_ _computation._ _Additionally,_ _it_ _computes_ _the_ _coefficients_ _of_ _its_ _inverse,_ _p_ ( _x_ ) _[−]_ [1] (mod _x_ _[L]_ ) _._


We now claim that this quantity is numerically unfeasible. We narrow down to the case when _**A**_ = _**I**_ is the
identity matrix. Note that this case is actually in some sense the most typical case: when discretizing the
continuous-time SSM to discrete-time by a step-size ∆, the discretized transition matrix _**A**_ is brought closer
to the identity. For example, with the Euler discretization _**A**_ = _**I**_ + ∆ _**A**_, we have _**A**_ _→_ _**I**_ as the step size
∆ _→_ 0.


**Lemma** **B.1.** _When_ _**A**_ = _**I**_ _,_ _the_ _fast_ _LSSL_ _algorithm_ _requires_ _computing_ _terms_ _exponentially_ _large_ _in_ _N_ _._


_Proof._ The characteristic polynomial of _**I**_ is


_p_ ( _x_ ) = det _|_ _**I**_ _−_ _x_ _**I**_ _|_ = (1 _−_ _x_ ) _[N]_ _._




 - _≈_ ~~_√_~~ 2 _[N]_
2




                - _N_
These coefficients have size up to _N_



_πN/_ 2 [.]



The inverse of _p_ ( _x_ ) has even larger coefficients. It can be calculated in closed form by the generalized binomial
formula:



(1 _−_ _x_ ) _[−][N]_ =



_∞_



_k_ =0




- _N_ + _k −_ 1�

_x_ _[k]_ _._

_k_



18


Taking this (mod _x_ _[L]_ ), the largest coefficient is

       - _N_ + _L −_ 2�       - _N_ + _L_

=

_L −_ 1 _N_ _−_




- - _N_ + _L −_ 2
=
_N_ _−_ 1




- [+ 1)]
= [(] _[L][ −]_ [1)(] _[L][ −]_ [2)] _[ . . .]_ [ (] _[L][ −]_ _[N]_



_._
( _N_ _−_ 1)!



When _L_ = _N_ _−_ 1 this is

�2( _N_ _−_ 1)
_N_ _−_ 1




_≈_ ~~_√_~~ [2][2] _[N]_



~~_√_~~



_πN_



already larger than the coefficients of (1 _−_ _x_ ) _[N]_, and only increases as _L_ grows.

##### **C S4 Algorithm Details**


This section proves the results of Section 3.3, providing complete details of our efficient algorithms for S4.


Appendices C.1 to C.3 prove Theorems 1 to 3 respectively.


**C.1** **NPLR** **Representations** **of** **HiPPO** **Matrices**


We first prove Theorem 1, showing that all HiPPO matrices for continuous-time memory fall under the S4
normal plus low-rank (NPLR) representation.


_Proof_ _of_ _Theorem_ _1._ We consider each of the three cases HiPPO-LagT, HiPPO-LegT, and HiPPO-LegS
separately. Note that the primary HiPPO matrix defined in this work (equation (2)) is the HiPPO-LegT
matrix.


**HiPPO-LagT.** The HiPPO-LagT matrix is simply



_**A**_ _nk_ =








0 _n < k_

_−_ [1] 2 _n_ = _k_

_−_ 1 _n > k_

















_._




_**A**_ = _−_



12 _. . ._
1 12
1 1 21
1 1 1 12
... ...



Adding the matrix of all [1] 2 [,] [which] [is] [rank] [1,] [yields]




[1] 2 _−_ [1] 2

[1] 2 _−_ [1] 2






 _._



2 2 2

12 _−_ [1] 2 _−_ [1] 2

12 12 _−_ [1] 2

1 1 1
2 2 2




_−_ [1]



2 [1] _−_ [1] 2




_−_










This matrix is now skew-symmetric. Skew-symmetric matrices are a particular case of normal matrices with
pure-imaginary eigenvalues.


Gu et al. [16] also consider a case of HiPPO corresponding to the generalized Laguerre polynomials that
generalizes the above HiPPO-LagT case. In this case, the matrix _**A**_ (up to conjugation by a diagonal matrix)
ends up being close to the above matrix, but with a different element on the diagonal. After adding the
rank-1 correction, it becomes the above skew-symmetric matrix plus a multiple of the identity. Thus after
diagonalization by the same matrix as in the LagT case, it is still reduced to diagonal plus low-rank (DPLR)
form, where the diagonal is now pure imaginary plus a real constant.


19


**HiPPO-LegS.** We restate the formula from equation (2) for convenience.








_**A**_ _nk_ = _−_



(2 _n_ + 1) [1] _[/]_ [2] (2 _k_ + 1) [1] _[/]_ [2] if _n > k_

_n_ + 1 if _n_ = _k_ _._

0 if _n < k_







Adding [1] 2 [(2] _[n]_ [ + 1)][1] _[/]_ [2][(2] _[k]_ [ + 1)][1] _[/]_ [2] [to] [the] [whole] [matrix] [gives]




_−_








12 [(2] _[n]_ [ + 1)][1] _[/]_ [2][(2] _[k]_ [ + 1)][1] _[/]_ [2] if _n > k_
12 if _n_ = _k_

_−_ [1] [(2] _[n]_ [ + 1)][1] _[/]_ [2][(2] _[k]_ [ + 1)][1] _[/]_ [2] if _n < k_








[1] 2 [(2] _[n]_ [ + 1)][1] _[/]_ [2][(2] _[k]_ [ + 1)][1] _[/]_ [2] if _n < k_



Note that this matrix is not skew-symmetric, but is [1] 2 _**[I]**_ [ +] _**[ S]**_ [where] _**[S]**_ [is] [a] [skew-symmetric] [matrix.] [This] [is]

diagonalizable by the same unitary matrix that diagonalizes _**S**_ .


**HiPPO-LegT.**


Up to the diagonal scaling, the LegT matrix is















_._




_**A**_ = _−_



1 _−_ 1 1 _−_ 1 _. . ._
1 1 _−_ 1 1
1 1 1 _−_ 1
1 1 1 1
... ...



By adding _−_ 1 to this matrix and then the matrix




2 2


2 2










the matrix becomes






_−_ 2 _−_ 2
2

_−_ 2
2 2














which is skew-symmetric. In fact, this matrix is the inverse of the Chebyshev Jacobi.



An alternative way to see this is as follows. The LegT matrix is the inverse of the matrix

 








_−_ 1 1 0

_−_ 1 1

_−_ 1 1

_−_ 1 _−_ 1











This can obviously be converted to a skew-symmetric matrix by adding a rank 2 term. The inverses of these
matrices are also rank-2 differences from each other by the Woodbury identity.



A final form is





















0 1 0 1

_−_ 1 0 1 0
0 _−_ 1 0 1

_−_ 1 0 _−_ 1 0








_−_ 1 1 _−_ 1 1

_−_ 1 _−_ 1 1 _−_ 1

_−_ 1 _−_ 1 _−_ 1 1

_−_ 1 _−_ 1 _−_ 1 _−_ 1






 =







 +







1 0 1 0
0 1 0 1
1 0 1 0
0 1 0 1







This has the advantage that the rank-2 correction is symmetric (like the others), but the normal skewsymmetric matrix is now 2-quasiseparable instead of 1-quasiseparable.


20


**C.2** **Computing** **the** **S4** **Recurrent** **View**


We prove Theorem 2 showing the efficiency of the S4 parameterization for computing one step of the recurrent
representation (Section 2.3).


Recall that without loss of generality, we can assume that the state matrix _**A**_ = **Λ** _−_ _**P Q**_ _[∗]_ is diagonal plus
low-rank (DPLR), potentially over C . Our goal in this section is to explicitly write out a closed form for the
discretized matrix _**A**_ .


Recall from equation (3) that


_**A**_ = ( _**I**_ _−_ ∆ _/_ 2 _·_ _**A**_ ) _[−]_ [1] ( _**I**_ + ∆ _/_ 2 _·_ _**A**_ )

_**B**_ = ( _**I**_ _−_ ∆ _/_ 2 _·_ _**A**_ ) _[−]_ [1] ∆ _**B**_ _._


We first simplify both terms in the definition of _**A**_ independently.


**Forward** **discretization.** The first term is essentially the Euler discretization motivated in Section 2.3.



_**I**_ + [∆]




[+] [∆]
2 _**[A]**_ [ =] _**[ I]**_ 2



2 [(] **[Λ]** _[ −]_ _**[P Q]**_ _[∗]_ [)]



= [∆]

2




- 2 
[+ (] **[Λ]** _[ −]_ _**[P Q]**_ _[∗]_ [)]
∆ _**[I]**_



= [∆]

2 _**[A]**_ **[0]**



where _**A**_ **0** is defined as the term in the final brackets.


**Backward** **discretization.** The second term is known as the Backward Euler’s method. Although this
inverse term is normally difficult to deal with, in the DPLR case we can simplify it using Woodbury’s Identity
(Proposition 4).




 - _−_ 1  = _**I**_ _−_ [∆]
2 _**[A]**_ 2




_**I**_ _−_ [∆]




[∆] - _−_ 1

2 [(] **[Λ]** _[ −]_ _**[P Q]**_ _[∗]_ [)]



= [2]

∆



= [2]

∆




- 2 - _−_ 1
∆ _[−]_ **[Λ]** [ +] _**[ P Q]**_ _[∗]_


- _**D**_ _−_ _**DP**_ ( _**I**_ + _**Q**_ _[∗]_ _**DP**_ ) _[−]_ [1] _**Q**_ _[∗]_ _**D**_



= [2]

∆ _**[A]**_ **[1]**



where _**D**_ = - ∆2 _[−]_ **[Λ]** - _−_ 1 and _**A**_ **1** is defined as the term in the final brackets. Note that (1 + _**Q**_ _∗_ _**DP**_ ) is actually
a scalar in the case when the low-rank term has rank 1.


**S4** **Recurrence.** Finally, the full bilinear discretization can be rewritten in terms of these matrices as


_**A**_ = _**A**_ **1** _**A**_ **0**



The discrete-time SSM (3) becomes



_**B**_ = [2] [= 2] _**[A]**_ **[1]** _**[B]**_ _[.]_

∆ _**[A]**_ **[1]** [∆] _**[B]**_


_xk_ = _**A**_ _xk−_ 1 + _**B**_ _uk_
= _**A**_ **1** _**A**_ **0** _xk−_ 1 + 2 _**A**_ **1** _**B**_ _uk_
_yk_ = _**C**_ _xk._



Note that _**A**_ **0** _,_ _**A**_ **1** are accessed only through matrix-vector multiplications. Since they are both DPLR, they
have _O_ ( _N_ ) matrix-vector multiplication, showing Theorem 2.


21


**C.3** **Computing** **the** **Convolutional** **View**


The most involved part of using SSMs efficiently is computing _**K**_ . This algorithm was sketched in Section 3.2
and is the main motivation for the S4 parameterization. In this section, we define the necessary intermediate
quantities and prove the main technical result.


The algorithm for Theorem 3 falls in roughly three stages, leading to Algorithm 1. Assuming _**A**_ has been
conjugated into diagonal plus low-rank form, we successively simplify the problem of computing _**K**_ by
applying the techniques outlined in Section 3.2.


**Remark** **C.1.** _**We**_ _**note**_ _**that**_ _**for**_ _**the**_ _**remainder**_ _**of**_ _**this**_ _**section**_ _,_ _we_ _transpose_ _**C**_ _to_ _be_ _a_ _column_ _vector_
_of_ _shape_ C _[N]_ _or_ C _[N]_ _[×]_ [1] _instead_ _of_ _matrix_ _or_ _row_ _vector_ C [1] _[×][N]_ _as_ _in_ (1) _._ _In_ _other_ _words_ _the_ _SSM_ _is_


_x_ _[′]_ ( _t_ ) = _**A**_ _x_ ( _t_ ) + _**B**_ _u_ ( _t_ )

(8)
_y_ ( _t_ ) = _**C**_ _[∗]_ _x_ ( _t_ ) + _**D**_ _u_ ( _t_ ) _._


_This_ _convention_ _is_ _made_ _so_ _that_ _**C**_ _has_ _the_ _same_ _shape_ _as_ _**B**_ _,_ _**P**_ _,_ _**Q**_ _and_ _simplifies_ _the_ _implementation_ _of_ _S4._


**Reduction** **0:** **Diagonalization** By Lemma 3.1, we can switch the representation by conjugating with
any unitary matrix. For the remainder of this section, we can assume that _**A**_ is (complex) diagonal plus
low-rank (DPLR).


Note that unlike diagonal matrices, a DPLR matrix does not lend itself to efficient computation of _**K**_ .
_∗_ ~~_i_~~
The reason is that _**K**_ computes terms _**C**_ _**A**_ _**B**_ which involve powers of the matrix _**A**_ . These are trivially
computable when _**A**_ is diagonal, but is no longer possible for even simple modifications to diagonal matrices
such as DPLR.


**Reduction** **1:** **SSM** **Generating** **Function** To address the problem of computing powers of _**A**_, we
introduce another technique. Instead of computing the SSM convolution filter _**K**_ directly, we introduce a
generating function on its coefficients and compute evaluations of it.


**Definition** **2** (SSM Generating Function) **.** _We_ _define_ _the_ _following_ _quantities:_


~~_∗_~~ ~~_∗_~~

  - _The_ SSM convolution function _is_ _K_ ( _**A**_ _,_ _**B**_ _,_ _**C**_ ) = ( _**C**_ _**B**_ _,_ _**C**_ _**AB**_ _, . . ._ ) _and_ _the_ _(truncated)_ _SSM_ _filter_ _of_
_length_ _L_
_KL_ ( _**A**_ _,_ _**B**_ _,_ _**C**_ ) = ( _**C**_ ~~_∗_~~ _**B**_ _,_ _**C**_ ~~_∗_~~ _**AB**_ _, . . .,_ _**C**_ ~~_∗_~~ _**A**_ ~~_L_~~ _−_ 1 _**B**_ ) _∈_ R _L_ (9)


  - _The_ SSM generating function _at_ _node_ _z_ _is_



_K_ ˆ( _z_ ; _**A**_ _,_ _**B**_ _,_ _**C**_ ) _∈_ C :=



_∞_



_i_ =0



_**C**_ ~~_∗_~~ _**A**_ ~~_i_~~ _**B**_ _zi_ = _**C**_ _∗_ ( _**I**_ _−_ _**A**_ _z_ ) _−_ 1 _**B**_ (10)



_and_ _the_ truncated SSM generating function _at_ _node_ _z_ _is_



_K_ ˆ _L_ ( _z_ ; _**A**_ _,_ _**B**_ _,_ _**C**_ ) _[∗]_ _∈_ C :=



_L−_ 1

- _**C**_ ~~_∗_~~ _**A**_ ~~_i_~~ _**B**_ _zi_ = _**C**_ _∗_ ( _**I**_ _−_ _**A**_ ~~_L_~~ _zL_ )( _**I**_ _−_ _**A**_ _z_ ) _−_ 1 _**B**_ (11)


_i_ =0




- _The_ _truncated_ _SSM_ _generating_ _function_ _at_ _nodes_ Ω _∈_ C _[M]_ _is_


_K_ ˆ _L_ (Ω; _**A**_ _,_ _**B**_ _,_ _**C**_ ) _∈_ C _[M]_ :=         - _K_ ˆ _L_ ( _ωk_ ; _**A**_ _,_ _**B**_ _,_ _**C**_ )�



(12)
_k∈_ [ _M_ ]



Intuitively, the generating function essentially converts the SSM convolution filter from the time domain to
frequency domain. Importantly, it preserves the same information, and the desired SSM convolution filter
can be recovered from evaluations of its generating function.


22


**Lemma C.2.** _The SSM function KL_ ( _**A**_ _,_ _**B**_ _,_ _**C**_ ) _can be computed from the SSM generating function_ _K_ [ˆ] _L_ (Ω; _**A**_ _,_ _**B**_ _,_ _**C**_ )
_at_ _the_ _roots_ _of_ _unity_ Ω= _{_ exp( _−_ 2 _πi_ _L_ _[k]_ [:] _[ k]_ _[∈]_ [[] _[L]_ []] _[}]_ _[stably]_ _[in]_ _[O]_ [(] _[L]_ [ log] _[ L]_ [)] _[operations.]_


_Proof._ For convenience define


_**K**_ = _KL_ ( _**A**_ _,_ _**B**_ _,_ _**C**_ )
_**K**_ **ˆ** = _K_ [ˆ] _L_ (Ω; _**A**_ _,_ _**B**_ _,_ _**C**_ )
_**K**_ **ˆ** ( _z_ ) = _K_ [ˆ] _L_ ( _z_ ; _**A**_ _,_ _**B**_ _,_ _**C**_ ) _._


Note that




_._



_**K**_ **ˆ** _j_ =



_L−_ 1



_k_ =0




   _**K**_ _k_ exp _−_ 2 _πi_ _[jk]_

_L_



Note that this is exactly the same as the Discrete Fourier Transform (DFT):


_**K**_ **ˆ** = _FL_ _**K**_ _._


Therefore _**K**_ can be recovered from _**K**_ **[ˆ]** with a single inverse DFT, which requires _O_ ( _L_ log _L_ ) operations with
the Fast Fourier Transform (FFT) algorithm.


**Reduction** **2:** **Woodbury** **Correction** The primary motivation of Definition 2 is that it turns _powers_ of
_**A**_ into a single _inverse_ of _**A**_ (equation (10)). While DPLR matrices cannot be powered efficiently due to the
low-rank term, they can be inverted efficiently by the well-known Woodbury identity.


**Proposition** **4** (Binomial Inverse Theorem or Woodbury matrix identity [15, 48]) **.** _Over_ _a_ _commutative_ _ring_
_R,_ _let_ _**A**_ _∈R_ _[N]_ _[×][N]_ _and_ _**U**_ _,_ _**V**_ _∈R_ _[N]_ _[×][p]_ _._ _Suppose_ _**A**_ _and_ _**A**_ + _**UV**_ _[∗]_ _are_ _invertible._ _Then_ _**I**_ _p_ + _**V**_ _[∗]_ _**A**_ _[−]_ [1] _**U**_ _∈R_ _[p][×][p]_

_is_ _invertible_ _and_


( _**A**_ + _**UV**_ _[∗]_ ) _[−]_ [1] = _**A**_ _[−]_ [1] _−_ _**A**_ _[−]_ [1] _**U**_ ( _**I**_ _p_ + _**V**_ _[∗]_ _**A**_ _[−]_ [1] _**U**_ ) _[−]_ [1] _**V**_ _[∗]_ _**A**_ _[−]_ [1]


With this identity, we can convert the SSM generating function on a DPLR matrix _**A**_ into one on just its
diagonal component.


**Lemma** **C.3.** _Let_ _**A**_ = **Λ** _−_ _**P Q**_ _[∗]_ _be_ _a_ _diagonal_ _plus_ _low-rank_ _representation._ _Then_ _for_ _any_ _root_ _of_ _unity_
_z_ _∈_ Ω _,_ _the_ _truncated_ _generating_ _function_ _satisfies_



2
_**K**_ **ˆ** ( _z_ ) =
1 + _z_




- _**C**_ **˜** _[∗]_ _**R**_ ( _z_ ) _**B**_ _−_ _**C**_ **[˜]** _[∗]_ _**R**_ ( _z_ ) _**P**_ (1 + _**Q**_ _[∗]_ _**R**_ ( _z_ ) _**P**_ ) _[−]_ [1] _**Q**_ _[∗]_ _**R**_ ( _z_ ) _**B**_ 


_**C**_ **˜** = ( _**I**_ _−_ _**A**_ ~~_L_~~ ) _∗_ _**C**_

         - 2 1 _−_ _z_         - _−_ 1
_**R**_ ( _z_ ; **Λ** ) = _._
∆ 1 + _z_ _[−]_ **[Λ]**


_Proof._ Directly expanding Definition 2 yields


_KL_ ( _z_ ; _**A**_ _,_ _**B**_ _,_ _**C**_ ) = _**C**_ ~~_∗_~~ _**B**_ + _**C**_ ~~_∗_~~ _**AB**_ _z_ + _· · ·_ + _**C**_ ~~_∗_~~ _**A**_ ~~_L_~~ _−_ 1 _**B**_ _zL−_ 1

= _**C**_ ~~_∗_~~ [�] _**I**_ _−_ _**A**_ ~~_L_~~ [��] _**I**_ _−_ _**A**_ _z_            - _−_ 1 _**B**_

= _**C**_ **[˜]** _[∗]_ [�] _**I**_ _−_ _**A**_ _z_            - _−_ 1 _**B**_


~~_L_~~ [�]
where _**C**_ **[˜]** _[∗]_ = _**C**_ _[∗]_ [�] _**I**_ _−_ _**A**_ .


23


We can now explicitly expand the discretized SSM matrices _**A**_ and _**B**_ back in terms of the original SSM
parameters _**A**_ and _**B**_ . Lemma C.4 provides an explicit formula, which allows further simplifying


_**C**_ **˜** _[∗]_ [�] _**I**_ _−_ _**A**_ _z_    - _−_ 1 _**B**_ = 2 _**C**_ **˜** _[∗]_    - 2 1 _−_ _z_    - _−_ 1 _**B**_
1 + _z_ ∆ 1 + _z_ _[−]_ _**[A]**_

2                 - 2 1 _−_ _z_                 - _−_ 1
= _**C**_ **˜** _[∗]_ _**B**_
1 + _z_ ∆ 1 + _z_ _[−]_ **[Λ]** [ +] _**[ P Q]**_ _[∗]_



2
=
1 + _z_




- _**C**_ **˜** _[∗]_ _**R**_ ( _z_ ) _**B**_ _−_ _**C**_ **[˜]** _[∗]_ _**R**_ ( _z_ ) _**P**_ (1 + _**Q**_ _[∗]_ _**R**_ ( _z_ ) _**P**_ ) _[−]_ [1] _**Q**_ _[∗]_ _**R**_ ( _z_ ) _**B**_ - _._



The last line applies the Woodbury Identity (Proposition 4) where _**R**_ ( _z_ ) = - ∆2 11+ _−zz_ _[−]_ **[Λ]** - _−_ 1.


The previous proof used the following self-contained result to back out the original SSM matrices from the
discretization.


**Lemma** **C.4.** _Let_ _**A**_ _,_ _**B**_ _be_ _the_ _SSM_ _matrices_ _**A**_ _,_ _**B**_ _discretized_ _by_ _the_ _bilinear_ _discretization_ _with_ _step_ _size_ ∆ _._
_Then_

_**C**_ _[∗]_ [�] _**I**_ _−_ _**Az**_        - _−_ 1 _**B**_ = 2∆ �2 [1] _[ −]_ _[z]_        - _−_ 1 _**B**_
1 + _z_ _**[C]**_ _[∗]_ 1 + _z_ _[−]_ [∆] _**[A]**_


_Proof._ Recall that the bilinear discretization that we use (equation (3)) is




 - _−_ 1 �
_**I**_ + [∆]
2 _**[A]**_ 2




  _**A**_ = _**I**_ _−_ [∆]




 2 _**[A]**_




  -  - _−_ 1
_**B**_ = _**I**_ _−_ [∆] ∆ _**B**_

2 _**[A]**_



The result is proved algebraic manipulations.



��
_**C**_ _[∗]_ [�] _**I**_ _−_ _**A**_ _z_ - _−_ 1 _**B**_ = _**C**_ _∗_ _**I**_ _−_ [∆]




[∆] - _−_ - _**I**_ _−_ [∆]

2 _**[A]**_ 2



_**B**_




[∆] - _−_ 1 � _**I**_ _−_ [∆]

2 _**[A]**_ 2


 -  
_−_ _**I**_ + [∆]
2 _**[A]**_ 2




 - _−_ 1 �
_**I**_ + [∆]
2 _**[A]**_ 2




 _**B**_
2 _**[A]**_




 _z_
2 _**[A]**_




- _−_ 1



��
= _**C**_ _[∗]_ _**I**_ _−_ [∆]




 -  - _−_ 1 �
_z_ _**I**_ _−_ [∆]
2 _**[A]**_ 2




   -   - _−_ 1
= _**C**_ _[∗]_ _**I**_ (1 _−_ _z_ ) _−_ [∆] ∆ _**B**_

2 _**[A]**_ [(1 +] _[ z]_ [)]




     ∆
=
1 _−_ _z_ _**[C]**_ _[∗]_



2 [1] _[−][z]_



1+ _z_



_**I**_ _−_ [∆] _**[A]**_




- _−_ 1



_**B**_



2∆  -  - _−_ 1
= 2 [1] _[ −]_ _[z]_ _[−]_ [∆] _**[A]**_ _**B**_
1 + _z_ _**[C]**_ _[∗]_ 1 + _z_ _**[I]**_




                             - ~~_L_~~ [�] _[∗]_
Note that in the S4 parameterization, instead of constantly computing _**C**_ **[˜]** = _**I**_ _−_ _**A**_ _**C**_, we can simply

reparameterize our parameters to learn _**C**_ **[˜]** directly instead of _**C**_, saving a minor computation cost and
simplifying the algorithm.


24


**Reduction** **3:** **Cauchy** **Kernel** We have reduced the original problem of computing _**K**_ to the problem
of computing the SSM generating function _K_ [ˆ] _L_ (Ω; _**A**_ _,_ _**B**_ _,_ _**C**_ ) in the case that _**A**_ is a diagonal matrix. We
show that this is exactly the same as a Cauchy kernel, which is a well-studied problem with fast and stable
numerical algorithms.


**Definition** **3.** _A_ _**Cauchy**_ _**matrix**_ _or_ _kernel_ _on_ _nodes_ Ω= ( _ωi_ ) _∈_ C _[M]_ _and_ Λ = ( _λj_ ) _∈_ C _[N]_ _is_


1
_**M**_ _∈_ C _[M]_ _[×][N]_ = _**M**_ (Ω _,_ Λ) = ( _**M**_ _ij_ ) _i∈_ [ _M_ ] _,j∈_ [ _N_ ] _**M**_ _ij_ = _ωi −_ _λj_ _._


_The_ _computation_ _time_ _of_ _a_ _Cauchy_ _matrix-vector_ _product_ _of_ _size_ _M_ _× N_ _is_ _denoted_ _by_ _C_ ( _M, N_ ) _._


Computing with Cauchy matrices is an extremely well-studied problem in numerical analysis, with both fast
arithmetic algorithms and fast numerical algorithms based on the famous Fast Multipole Method (FMM)

[29, 30, 31].


**Proposition** **5** (Cauchy) **.** _A_ _Cauchy_ _kernel_ _requires_ _O_ ( _M_ + _N_ ) _space,_ _and_ _operation_ _count_



_C_ ( _M, N_ ) =








_O_ ( _MN_ ) _naively_
_O_ �( _M_ + _N_ ) log [2] ( _M_ + _N_ )� _in_ _exact_ _arithmetic_
_O_ �( _M_ + _N_ ) log( _M_ + _N_ ) log [1] - _numerically_ _to_








[1] _ε_ - _numerically_ _to_ _precision_ _ε._



**Corollary** **C.5.** _Evaluating_ _**Q**_ _[∗]_ _**R**_ (Ω; Λ) _**P**_ _(defined_ _in_ _Lemma_ _C.3)_ _for_ _any_ _set_ _of_ _nodes_ Ω _∈_ C _[L]_ _,_ _diagonal_
_matrix_ Λ _,_ _and_ _vectors_ _**P**_ _,_ _**Q**_ _can_ _be_ _computed_ _in_ _C_ ( _L, N_ ) _operations_ _and_ _O_ ( _L_ + _N_ ) _space,_ _where_ _C_ ( _L, N_ ) =
_O_ ˜( _L_ + _N_ ) _is_ _the_ _cost_ _of_ _a_ _Cauchy_ _matrix-vector_ _multiplication._


_Proof._ For any fixed _ω_ _∈_ Ω, we want to compute [�] _j_ _ωq−j_ _[∗][p]_ _λ_ _[j]_ _j_ [.] [Computing] [this] [over] [all] _[ω][i]_ [is] [therefore] [exactly] [a]

Cauchy matrix-vector multiplication.


This completes the proof of Theorem 3. In Algorithm 1, note that the work is dominated by Step 2, which
has a constant number of calls to a black-box Cauchy kernel, with complexity given by Proposition 5.

##### **D Experiment Details and Full Results**


This section contains full experimental procedures and extended results and citations for our experimental
evaluation in Section 4. Appendix D.1 corresponds to benchmarking results in Section 4.1, Appendix D.2
corresponds to LRD experiments (LRA and Speech Commands) in Section 4.2, and Appendix D.3 corresponds
to the general sequence modeling experiments (generation, image classification, forecasting) in Section 4.3.


**D.1** **Benchmarking**


Benchmarking results from Table 2 and Table 3 were tested on a single A100 GPU.


**Benchmarks** **against** **LSSL** For a given dimension _H_, a single LSSL or S4 layer was constructed with _H_
hidden features. For LSSL, the state size _N_ was set to _H_ as done in [18]. For S4, the state size _N_ was set to
parameter-match the LSSL, which was a state size of _[N]_ 4 [due] [to] [differences] [in] [the] [parameterization.] [Table] [2]

benchmarks a single forward+backward pass of a single layer.


25


Table 10: Full results for the Long Range Arena (LRA) benchmark for long-range dependencies in sequence models.
(Top): Original Transformer variants in LRA. (Bottom): Other models reported in the literature.


Model ListOps Text Retrieval Image Pathfinder Path-X Avg


Random 10.00 50.00 50.00 10.00 50.00 50.00 36.67


Transformer 36.37 64.27 57.46 42.44 71.40 53.66
Local Attention 15.82 52.98 53.39 41.46 66.63 46.71
Sparse Trans. 17.07 63.58 59.59 44.24 71.71 51.03
Longformer 35.63 62.85 56.89 42.22 69.71 52.88
Linformer 35.70 53.94 52.27 38.56 76.34 51.14
Reformer 37.27 56.10 53.40 38.07 68.50 50.56
Sinkhorn Trans. 33.67 61.20 53.83 41.23 67.45 51.23
Synthesizer 36.99 61.68 54.67 41.61 69.45 52.40
BigBird 36.05 64.02 59.29 40.83 74.87 54.17
Linear Trans. 16.13 65.90 53.09 42.34 75.30 50.46
Performer 18.01 65.40 53.82 42.77 77.05 51.18


FNet 35.33 65.11 59.61 38.67 77.80 54.42
Nystr¨omformer 37.15 65.52 79.56 41.58 70.94 57.46
Luna-256 37.25 64.57 79.29 47.38 77.72 59.37
**S4** (original) 58.35 76.02 87.09 87.26 86.05 88.10 80.48
**S4** (updated) **59.60** **86.82** **90.90** **88.65** **94.20** **96.35** **86.09**


**Benchmarks** **against** **Efficient** **Transformers** Following [40], the Transformer models had 4 layers,
hidden dimension 256 with 4 heads, query/key/value projection dimension 128, and batch size 32, for a total
of roughly 600 _k_ parameters. The S4 model was parameter tied while keeping the depth and hidden dimension
constant (leading to a state size of _N_ = 256).


We note that the relative orderings of these methods can vary depending on the exact hyperparameter
settings.


**D.2** **Long-Range** **Dependencies**


This section includes information for reproducing our experiments on the Long-Range Arena and Speech
Commands long-range dependency tasks.


**Long** **Range** **Arena** Table 10 contains extended results table with all 11 methods considered in [40].


For the S4 model, hyperparameters for all datasets are reported in Table 11. For all datasets, we used the
AdamW optimizer with a constant learning rate schedule with decay on validation plateau. However, the
learning rate on HiPPO parameters (in particular **Λ** _,_ _**P**_ _,_ _**Q**_ _,_ _**B**_ _,_ _**C**_ _,_ ∆) were reduced to a maximum starting
LR of 0 _._ 001, which improves stability since the HiPPO equation is crucial to performance.


The S4 state size was always fixed to _N_ = 64.


As S4 is a sequence-to-sequence model with output shape (batch, length, dimension) and LRA tasks are
classification, mean pooling along the length dimension was applied after the last layer.


We note that most of these results were trained for far longer than what was necessary to achieve SotA results
(e.g., the `Image` task reaches SotA in 1 epoch). Results often keep improving with longer training times.


**Updated** **results.** The above hyperparameters describe the results reported in the original paper, shown in
Table 10, which have since been improved. See Appendix D.5.


**Hardware.** All models were run on single GPU. Some tasks used an A100 GPU (notably, the Path-X
experiments), which has a larger max memory of 40Gb. To reproduce these on smaller GPUs, the batch size
can be reduced or gradients can be accumulated for two batches.


26


Table 11: The values of the best hyperparameters found for classification datasets; LRA (Top) and images/speech
(Bottom). LR is learning rate and WD is weight decay. BN and LN refer to Batch Normalization and Layer
Normalization.


**Depth** **Features** _H_ **Norm** **Pre-norm** **Dropout** **LR** **Batch** **Size** **Epochs** **WD** **Patience**


**ListOps** 6 128 BN False 0 0.01 100 50 0.01 5
**Text** 4 64 BN True 0 0.001 50 20 0 5
**Retrieval** 6 256 BN True 0 0.002 64 20 0 20
**Image** 6 512 LN False 0.2 0.004 50 200 0.01 20
**Pathfinder** 6 256 BN True 0.1 0.004 100 200 0 10
**Path-X** 6 256 BN True 0.0 0.0005 32 100 0 20


**CIFAR-10** 6 1024 LN False 0.25 0.01 50 200 0.01 20


**Speech** **Commands** **(MFCC)** 4 256 LN False 0.2 0.01 100 50 0 5
**Speech** **Commands** **(Raw)** 6 128 BN True 0.1 0.01 20 150 0 10


**Speech** **Commands** We provide details of sweeps run for baseline methods run by us—numbers for all
others method are taken from Gu et al. [18]. The best hyperparameters used for S4 are included in Table 11.


_Transformer_ _[44]_ For MFCC, we swept the number of model layers _{_ 2 _,_ 4 _}_, dropout _{_ 0 _,_ 0 _._ 1 _}_ and learning rates
_{_ 0 _._ 001 _,_ 0 _._ 0005 _}_ . We used 8 attention heads, model dimension 128, prenorm, positional encodings, and trained
for 150 epochs with a batch size of 100. For Raw, the Transformer model’s memory usage made training
impossible.


_Performer_ _[8]_ For MFCC, we swept the number of model layers _{_ 2 _,_ 4 _}_, dropout _{_ 0 _,_ 0 _._ 1 _}_ and learning rates
_{_ 0 _._ 001 _,_ 0 _._ 0005 _}_ . We used 8 attention heads, model dimension 128, prenorm, positional encodings, and trained
for 150 epochs with a batch size of 100. For Raw, we used a model dimension of 128, 4 attention heads,
prenorm, and a batch size of 16. We reduced the number of model layers to 4, so the model would fit on the
single GPU. We trained for 100 epochs with a learning rate of 0 _._ 001 and no dropout.


_ExpRNN_ _[24]_ For MFCC, we swept hidden sizes _{_ 256 _,_ 512 _}_ and learning rates _{_ 0 _._ 001 _,_ 0 _._ 002 _,_ 0 _._ 0005 _}_ . Training
was run for 200 epochs, with a single layer model using a batch size of 100. For Raw, we swept hidden sizes
_{_ 32 _,_ 64 _}_ and learning rates _{_ 0 _._ 001 _,_ 0 _._ 0005 _}_ (however, ExpRNN failed to learn).


_LipschitzRNN_ _[13]_ For MFCC, we swept hidden sizes _{_ 256 _,_ 512 _}_ and learning rates _{_ 0 _._ 001 _,_ 0 _._ 002 _,_ 0 _._ 0005 _}_ .
Training was run for 150 epochs, with a single layer model using a batch size of 100. For Raw, we found that
LipschitzRNN was too slow to train on a single GPU (requiring a full day for 1 epoch of training alone).


_WaveGAN Discriminator [11]_ The WaveGAN-D in Table 5 is actually our improved version of the discriminator
network from the recent WaveGAN model for speech [11]. This CNN actually did not work well out-of-the-box,
and we added several features to help it perform better. The final model is highly specialized compared to
our model, and includes:


  - Downsampling or pooling between layers, induced by strided convolutions, that decrease the sequence
length between layers.


  - A global fully-connected output layer; thus the model only works for one input sequence length and
does not work on MFCC features or the frequency-shift setting in Table 5.


  - Batch Normalization is essential, whereas S4 works equally well with either Batch Normalization or
Layer Normalization.


  - Almost 90 _×_ as many parameters as the S4 model (26 _._ 3M vs. 0 _._ 3M).


**D.3** **General** **Sequence** **Modeling**


This subsection corresponds to the experiments in Section 4.3. Because of the number of experiments
in this section, we use subsubsection dividers for different tasks to make it easier to follow: CIFAR-10
density estimation (Appendix D.3.1), WikiText-103 language modeling (Appendix D.3.2), autoregressive


27


generation (Appendix D.3.3), sequential image classification (Appendix D.3.4), and time-series forecasting
(Appendix D.3.5).


**D.3.1** **CIFAR** **Density** **Estimation**


This task used a different backbone than the rest of our experiments. We used blocks of alternating S4 layers
and position-wise feed-forward layers (in the style of Transformer blocks). Each feed-forward intermediate
dimension was set to 2 _×_ the hidden size of the incoming S4 layer. Similar to Salimans et al. [39], we used a
UNet-style backbone consisting of _B_ identical blocks followed by a downsampling layer. The downsampling
rates were 3 _,_ 4 _,_ 4 (the 3 chosen because the sequence consists of RGB pixels). The base model had _B_ = 8
with starting hidden dimension 128, while the large model had _B_ = 16 with starting hidden dimension 192.


We experimented with both the mixture of logistics from [39] as well as a simpler 256-way categorical loss. We
found they were pretty close and ended up using the simpler softmax loss along with using input embeddings.


We used the LAMB optimizer with learning rate 0.005. The base model had no dropout, while the large
model had dropout 0.1 before the linear layers inside the S4 and FF blocks.


**D.3.2** **WikiText-103** **Language** **Modeling**


The RNN baselines included in Table 8 are the AWD-QRNN [27], an efficient linear gated RNN, and the
LSTM + Cache + Hebbian + MbPA [33], the best performing pure RNN in the literature. The CNN
baselines are the CNN with GLU activations [9], the TrellisNet [4], Dynamic Convolutions [49], and TaLK
Convolutions [26].


The Transformer baseline is [2], which uses Adaptive Inputs with a tied Adaptive Softmax. This model is a
standard high-performing Transformer baseline on this benchmark, used for example by Lioutas and Guo

[26] and many more.


Our S4 model uses the same Transformer backbone as in [2]. The model consists of 16 blocks of S4 layers
alternated with position-wise feedforward layers, with a feature dimension of 1024. Because our S4 layer
has around 1/4 the number of parameters as a self-attention layer with the same dimension, we made two
modifications to match the parameter count better: (i) we used a GLU activation after the S4 linear layer
(Section 3.4) (ii) we used two S4 layers per block. Blocks use Layer Normalization in the pre-norm position.
The embedding and softmax layers were the Adaptive Embedding from [2] with standard cutoffs 20000, 40000,
200000.


Evaluation was performed similarly to the basic setting in [2], Table 5, which uses sliding non-overlapping
windows. Other settings are reported in [2] that include more context at training and evaluation time and
improves the score. Because such evaluation protocols are orthogonal to the basic model, we do not consider
them and report the base score from [2] Table 5.


Instead of SGD+Momentum with multiple cosine learning rate annealing cycles, our S4 model was trained
with the simpler AdamW optimizer with a single cosine learning rate cycle with a maximum of 800000 steps.
The initial learning rate was set to 0.0005. We used 8 A100 GPUs with a batch size of 1 per gpu and context
size 8192. We used no gradient clipping and a weight decay of 0.1. Unlike [2] which specified different dropout
rates for different parameters, we used a constant dropout rate of 0.25 throughout the network, including
before every linear layer and on the residual branches.


**D.3.3** **Autoregressive** **Generation** **Speed**


**Protocol.** To account for different model sizes and memory requirements for each method, we benchmark
generation speed by throughput, measured in images per second (Table 7) or tokens per second (Table 8).
Each model generates images on a single _A_ 100 GPU, maximizing batch size to fit in memory. (For CIFAR-10
generation we limited memory to 16Gb, to be more comparable to the Transformer and Linear Transformer
results reported from [22].)


28


Table 12: ( **Pixel-level** **image** **classification.** ) Citations refer to the original model; additional citation indicates
work from which this baseline is reported.


Model sMNIST pMNIST sCIFAR


Transformer [42, 44] 98.9 97.9 62.2


CKConv [35] 99.32 98.54 63.74
TrellisNet [4] 99.20 98.13 73.42
TCN [3] 99.0 97.2          

LSTM [17, 21] 98.9 95.11 63.01
r-LSTM [42] 98.4 95.2 72.2
Dilated GRU [5] 99.0 94.6               Dilated RNN [5] 98.0 96.1               IndRNN [25] 99.0 96.0           expRNN [24] 98.7 96.6           UR-LSTM 99.28 96.96 71.00
UR-GRU [17] 99.27 96.51 74.4
LMU [45]         - 97.15         HiPPO-RNN [16] 98.9 98.3 61.1
UNIcoRNN [38]           - 98.4           LMUFFT [7]          - 98.49          LipschitzRNN [13] 99.4 96.3 64.2


**S4** **99.63** **98.70** **91.13**


**Baselines.** The Transformer and Linear Transformer baselines reported in Table 7 are the results reported
directly from Katharopoulos et al. [22]. Note that the Transformer number is the one in their Appendix,
which implements the optimized cached implementation of self-attention.


For all other baseline models, we used open source implementations of the models to benchmark generation
speed. For the PixelCNN++, we used the fast cached version by Ramachandran et al. [34], which sped
up generation by orders of magnitude from the naive implementation. This code was only available in
TensorFlow, which may have slight differences compared to the rest of the baselines which were implemented
in PyTorch.


We were unable to run the Sparse Transformer [6] model due to issues with their custom CUDA implementation
of the sparse attention kernel, which we were unable to resolve.


The Transformer baseline from Table 8 was run using a modified GPT-2 backbone from the HuggingFace
repository, configured to recreate the architecture reported in [2]. These numbers are actually slightly
favorable to the baseline, as we did not include the timing of the embedding or softmax layers, whereas the
number reported for S4 is the full model.


**D.3.4** **Pixel-Level** **Sequential** **Image** **Classification**


Our models were trained with the AdamW optimizer for up to 200 epochs. Hyperparameters for the CIFAR-10
model is reported in Table 11.


For our comparisons against ResNet-18, the main differences between the base models are that S4 uses
LayerNorm by default while ResNet uses BatchNorm. The last ablation in Section 4.3 swaps the normalization
type, using BatchNorm for S4 and LayerNorm for ResNet, to ablate this architectural difference. The
experiments with augmentation take the base model and train with mild data augmentation: horizontal flips
and random crops (with symmetric padding).


29


Context Forecast


Forecast

|Col1|Col2|Col3|S4<br>S4|Col5|Col6|
|---|---|---|---|---|---|
||||S4S4|S4S4|S4S4|
||||S4|S4|S4|
|S4|S4|S4||||
|S4|S4|S4||||
|S4|S4|S4||||



Context


Figure 5: Comparison of S4 and specialized time-series models for forecasting tasks. ( _Top_ _Left_ ) The forecasting task
involves predicting future values of a time-series given past context. ( _Bottom_ _Left_ ) We perform simple forecasting
using a sequence model such as S4 as a black box. ( _Right_ ) Informer uses an encoder-decoder architecture designed
specifically for forecasting problems involving a customized attention module (figure taken from Zhou et al. [50]).


**D.3.5** **Time** **Series** **Forecasting** **compared** **to** **Informer**


We include a simple figure (Fig. 5) contrasting the architecture of S4 against that of the Informer [50].


In Fig. 5, the goal is to forecast a contiguous range of future predictions (Green, length _F_ ) given a range of
past context (Blue, length _C_ ). We simply concatenate the entire context with a sequence of masks set to the
length of the forecast window. This input is a single sequence of length _C_ + _F_ that is run through the same
simple deep S4 model used throughout this work, which maps to an output of length _C_ + _F_ . We then use
just the last _F_ outputs as the forecasted predictions.


Tables 13 and 14 contain full results on all 50 settings considered by Zhou et al. [50]. S4 sets the best results
on 40 out of 50 of these settings.


**D.4** **Visualizations**


We visualize the convolutional filter _K_ [¯] learned by S4 for the Pathfinder and CIFAR-10 tasks in Appendix D.4.


**D.5** **Reproduction**


Since the first version of this paper, several experiments have been updated. Please read the corresponding
paragraph below before citing LRA or SC results.


**Long Range Arena** Follow-ups to this paper expanded the theoretical understanding of S4 while improving
some results. The results reported in Table 4 have been updated to results from the papers [19, 20]. More
specifically, the method S4-LegS in those works refers to the _same_ _model_ presented in this paper, with the


30


