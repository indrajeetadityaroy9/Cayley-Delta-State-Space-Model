experiment_name: wikitext_default
seed: 42

data:
  dataset_name: wikitext
  dataset_config: wikitext-103-raw-v1
  context_length: 1024
  batch_size: 8
  num_workers: 8

model:
  d_model: 768
  n_layers: 12

training:
  epochs: 20
  grad_accum_steps: 8
  base_lr: 6e-4
  weight_decay: 0.1
  betas: [0.9, 0.95]
  warmup_steps: 500
  grad_clip: 1.0
  min_lr_ratio: 0.0

checkpoint_dir: checkpoints
